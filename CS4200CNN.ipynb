{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alimomennasab/ASL-Translator/blob/main/CS4200CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBwNFWBeBHfM",
        "outputId": "4af7f956-1898-4ee6-aa45-cc91cdf94d05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Preprocessing"
      ],
      "metadata": {
        "id": "3yFDo3tx5LNW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bXU3A91_fZtr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ed09011-a70c-4db1-b484-fd341f8cfc11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleared folder\n",
            "Found 6812 total videos\n",
            "Using 2 workers\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Stacking videos: 100%|██████████| 6812/6812 [1:28:40<00:00,  1.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed. 59 videos failed.\n",
            "\n",
            "List of all failed videos:\n",
            "Could not open video: /content/drive/MyDrive/WLASL_300/train_augmented/man/34738_AUG4_zoom_mirror.mp4\n",
            "Could not open video: /content/drive/MyDrive/WLASL_300/train_augmented/man/34733_AUG2_bright_zoom.mp4\n",
            "Could not open video: /content/drive/MyDrive/WLASL_300/train_augmented/man/34744_AUG3_zoom_mirror.mp4\n",
            "Could not open video: /content/drive/MyDrive/WLASL_300/train_augmented/paper/41026_AUG2_gray_slow.mp4\n",
            "Could not open video: /content/drive/MyDrive/WLASL_300/train_augmented/paint/40836_AUG1_zoom_bright.mp4\n",
            "Could not open video: /content/drive/MyDrive/WLASL_300/train_augmented/paint/40834_AUG1_slow_mirror.mp4\n",
            "Could not open video: /content/drive/MyDrive/WLASL_300/train_augmented/paint/68125_AUG5_bright_mirror.mp4\n",
            "Could not open video: /content/drive/MyDrive/WLASL_300/train_augmented/order/40171_AUG5_fast_zoom.mp4\n",
            "Could not open video: /content/drive/MyDrive/WLASL_300/train_augmented/like/33279_AUG5_mirror_slow.mp4\n",
            "Could not open video: /content/drive/MyDrive/WLASL_300/train_augmented/many/34824_AUG4_gray_zoom.mp4\n",
            "Could not open video: /content/drive/MyDrive/WLASL_300/train_augmented/many/34835_AUG3_zoom_bright.mp4\n",
            "Could not open video: /content/drive/MyDrive/WLASL_300/train_augmented/many/34826_AUG1_slow_zoom.mp4\n",
            "Could not open video: /content/drive/MyDrive/WLASL_300/train_augmented/many/34825_AUG1_zoom_gray.mp4\n",
            "Could not open video: /content/drive/MyDrive/WLASL_300/train_augmented/no/38527_AUG5_gray_mirror.mp4\n",
            "Could not open video: /content/drive/MyDrive/WLASL_300/train_augmented/read/46268_AUG1_mirror_zoom.mp4\n",
            "Could not open video: /content/drive/MyDrive/WLASL_300/train_augmented/read/46267_AUG1_mirror_bright.mp4\n",
            "Could not open video: /content/drive/MyDrive/WLASL_300/train_augmented/read/46273_AUG1_gray_slow.mp4\n",
            "Could not open video: /content/drive/MyDrive/WLASL_300/train_augmented/read/46267_AUG2_bright_slow.mp4\n",
            "Could not open video: /content/drive/MyDrive/WLASL_300/train_augmented/read/46268_AUG2_slow_bright.mp4\n",
            "Could not open video: /content/drive/MyDrive/WLASL_300/train_augmented/language/32160_AUG1_gray_mirror.mp4\n",
            "Could not open video: /content/drive/MyDrive/WLASL_300/train_augmented/old/39626_AUG1_zoom_bright.mp4\n",
            "Could not open video: /content/drive/MyDrive/WLASL_300/train_augmented/old/39633_AUG3_zoom_bright.mp4\n",
            "Could not open video: /content/drive/MyDrive/WLASL_300/train_augmented/jump/31315_AUG5_mirror_zoom.mp4\n",
            "Could not open video: /content/drive/MyDrive/WLASL_300/train_augmented/jump/69380_AUG4_slow_mirror.mp4\n",
            "Could not open video: /content/drive/MyDrive/WLASL_300/train_augmented/pants/40986_AUG1_mirror_gray.mp4\n",
            "Could not open video: /content/drive/MyDrive/WLASL_300/train_augmented/pants/40985_AUG2_mirror_zoom.mp4\n",
            "Could not open video: /content/drive/MyDrive/WLASL_300/train_augmented/pants/40993_AUG2_mirror_fast.mp4\n",
            "Could not open video: /content/drive/MyDrive/WLASL_300/train_augmented/pants/40986_AUG5_mirror_bright.mp4\n",
            "Could not open video: /content/drive/MyDrive/WLASL_300/train_augmented/kill/31651_AUG2_gray_zoom.mp4\n",
            "Could not open video: /content/drive/MyDrive/WLASL_300/train_augmented/kill/31649_AUG3_slow_bright.mp4\n",
            "Could not open video: /content/drive/MyDrive/WLASL_300/train_augmented/now/39001_AUG3_gray_zoom.mp4\n",
            "Could not open video: /content/drive/MyDrive/WLASL_300/train_augmented/now/39002_AUG1_zoom_fast.mp4\n",
            "Could not open video: /content/drive/MyDrive/WLASL_300/train_augmented/make/34579_AUG1_zoom_mirror.mp4\n",
            "Could not open video: /content/drive/MyDrive/WLASL_300/train_augmented/make/34579_AUG3_slow_zoom.mp4\n",
            "Could not open video: /content/drive/MyDrive/WLASL_300/train_augmented/make/34578_AUG2_fast_zoom.mp4\n",
            "Could not open video: /content/drive/MyDrive/WLASL_300/train_augmented/make/34580_AUG2_fast_zoom.mp4\n",
            "Could not open video: /content/drive/MyDrive/WLASL_300/train_augmented/make/34579_AUG4_mirror_zoom.mp4\n",
            "Could not open video: /content/drive/MyDrive/WLASL_300/train_augmented/make/34580_AUG5_mirror_gray.mp4\n",
            "Could not open video: /content/drive/MyDrive/WLASL_300/train_augmented/make/34578_AUG3_bright_fast.mp4\n",
            "Could not open video: /content/drive/MyDrive/WLASL_300/train_augmented/list/33474_AUG1_gray_mirror.mp4\n",
            "Could not open video: /content/drive/MyDrive/WLASL_300/train_augmented/list/33477_AUG4_mirror_slow.mp4\n",
            "Could not open video: /content/drive/MyDrive/WLASL_300/train_augmented/list/33484_AUG5_zoom_slow.mp4\n",
            "Could not open video: /content/drive/MyDrive/WLASL_300/train_augmented/list/33482_AUG3_bright_mirror.mp4\n",
            "Could not open video: /content/drive/MyDrive/WLASL_300/train_augmented/lose/34005_AUG4_zoom_mirror.mp4\n",
            "Could not open video: /content/drive/MyDrive/WLASL_300/train_augmented/lose/34015_AUG3_zoom_gray.mp4\n",
            "Could not open video: /content/drive/MyDrive/WLASL_300/train_augmented/lose/34005_AUG5_fast_zoom.mp4\n",
            "Could not open video: /content/drive/MyDrive/WLASL_300/train_augmented/rabbit/45845_AUG1_fast_mirror.mp4\n",
            "Could not open video: /content/drive/MyDrive/WLASL_300/train_augmented/rabbit/45839_AUG3_fast_mirror.mp4\n",
            "Could not open video: /content/drive/MyDrive/WLASL_300/train_augmented/rabbit/45838_AUG4_zoom_bright.mp4\n",
            "Could not open video: /content/drive/MyDrive/WLASL_300/train_augmented/know/31906_AUG5_gray_mirror.mp4\n",
            "Could not open video: /content/drive/MyDrive/WLASL_300/train_augmented/know/31907_AUG1_zoom_mirror.mp4\n",
            "Could not open video: /content/drive/MyDrive/WLASL_300/train_augmented/know/31905_AUG3_bright_mirror.mp4\n",
            "Could not open video: /content/drive/MyDrive/WLASL_300/train_augmented/know/31899_AUG2_fast_bright.mp4\n",
            "Could not open video: /content/drive/MyDrive/WLASL_300/train_augmented/none/69412_AUG2_zoom_bright.mp4\n",
            "Could not open video: /content/drive/MyDrive/WLASL_300/train_augmented/none/68111_AUG3_zoom_mirror.mp4\n",
            "Could not open video: /content/drive/MyDrive/WLASL_300/train_augmented/none/68111_AUG4_slow_mirror.mp4\n",
            "Could not open video: /content/drive/MyDrive/WLASL_300/train_augmented/none/69412_AUG5_zoom_gray.mp4\n",
            "Could not open video: /content/drive/MyDrive/WLASL_300/train_augmented/orange/40119_AUG1_slow_bright.mp4\n",
            "Could not open video: /content/drive/MyDrive/WLASL_300/train_augmented/orange/40130_AUG5_zoom_bright.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# import os\n",
        "# import cv2\n",
        "# import numpy as np\n",
        "# import shutil\n",
        "# from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "# from tqdm import tqdm\n",
        "# from pathlib import Path\n",
        "\n",
        "# IMG_SIZE = 256\n",
        "# FRAME_SKIP = 2\n",
        "# dataset_dir = Path(\"/content/drive/MyDrive/WLASL_300/train_augmented\")\n",
        "# stacked_dataset_dir = Path(\"/content/drive/MyDrive/WLASL_300/train_augmented_stacked\")\n",
        "\n",
        "# # !rsync -a --progress \"/content/drive/MyDrive/WLASL_300/train_augmented/\" \"/content/train_augmented/\"\n",
        "# # dataset_dir = Path(\"/content/train_augmented\")\n",
        "# # stacked_dataset_dir = Path(\"/content/train_augmented_stacked\")\n",
        "\n",
        "\n",
        "# # Clear stacked dataset directory\n",
        "# if os.path.exists(stacked_dataset_dir):\n",
        "#     for sub in os.listdir(stacked_dataset_dir):\n",
        "#         path = os.path.join(stacked_dataset_dir, sub)\n",
        "#         if os.path.isdir(path):\n",
        "#             shutil.rmtree(path)\n",
        "#         else:\n",
        "#             os.remove(path)\n",
        "# else:\n",
        "#     os.makedirs(stacked_dataset_dir)\n",
        "# print(\"Cleared folder\")\n",
        "\n",
        "# def stack_frames_max(frames_bgr, resize=(IMG_SIZE, IMG_SIZE), frame_skip=FRAME_SKIP):\n",
        "#     if not frames_bgr:\n",
        "#         return None\n",
        "#     frames_bgr = frames_bgr[::max(1, frame_skip)]\n",
        "#     acc = None\n",
        "#     for f in frames_bgr:\n",
        "#         f = cv2.resize(f, resize)\n",
        "#         if acc is None:\n",
        "#             acc = f.astype(np.float32)\n",
        "#         else:\n",
        "#             acc = np.maximum(acc, f.astype(np.float32))\n",
        "#     return acc.astype(np.uint8)\n",
        "\n",
        "# def process_video(video_path, class_dir_name):\n",
        "#     save_dir = stacked_dataset_dir / class_dir_name\n",
        "#     save_dir.mkdir(parents=True, exist_ok=True)\n",
        "#     out_path = save_dir / f\"{video_path.stem}_stacked.png\"\n",
        "\n",
        "#     cap = cv2.VideoCapture(str(video_path))\n",
        "#     if not cap.isOpened():\n",
        "#         return f\"Could not open video: {video_path}\"\n",
        "\n",
        "#     frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "#     fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "#     width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "#     height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "#     if frame_count == 0:\n",
        "#         cap.release()\n",
        "#         return f\"No frames found in: {video_path}\"\n",
        "\n",
        "#     frames = []\n",
        "#     while cap.isOpened():\n",
        "#         ret, frame = cap.read()\n",
        "#         if not ret:\n",
        "#             break\n",
        "#         frames.append(frame)\n",
        "#     cap.release()\n",
        "\n",
        "#     if len(frames) == 0:\n",
        "#         return (f\"No frames read from {video_path}\\n\"\n",
        "#                 f\"    Metadata: {frame_count} frames, {fps:.2f} FPS, {width}x{height}\\n\"\n",
        "#                 f\"    Possible causes:\\n\"\n",
        "#                 f\"      - Codec not supported by OpenCV in this environment\\n\"\n",
        "#                 f\"      - Corrupt video file or 0-byte content\\n\"\n",
        "#                 f\"      - File path issue or permission denied\")\n",
        "#     elif len(frames) < frame_count * 0.1:\n",
        "#         print(f\"Warning: Only read {len(frames)} / {frame_count} frames from {video_path} (partial read)\")\n",
        "\n",
        "#     stacked_img = stack_frames_max(frames)\n",
        "#     if stacked_img is None:\n",
        "#         return f\"Stacking failed for {video_path}\"\n",
        "\n",
        "#     success = cv2.imwrite(str(out_path), stacked_img)\n",
        "#     if not success:\n",
        "#         return f\"Failed to write stacked image: {out_path}\"\n",
        "\n",
        "#     return None  # success\n",
        "\n",
        "# # Collect all videos\n",
        "# video_tasks = []\n",
        "# for class_dir in dataset_dir.iterdir():\n",
        "#     if not class_dir.is_dir():\n",
        "#         continue\n",
        "#     for video_path in class_dir.glob(\"*.mp4\"):\n",
        "#         video_tasks.append((video_path, class_dir.name))\n",
        "\n",
        "# print(f\"Found {len(video_tasks)} total videos\")\n",
        "\n",
        "# # Parallel ThreadPoolExecutor\n",
        "# max_workers = os.cpu_count()\n",
        "# print(f\"Using {max_workers} workers\")\n",
        "# errors = []\n",
        "# with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "#     futures = [executor.submit(process_video, v, c) for v, c in video_tasks]\n",
        "#     for f in tqdm(as_completed(futures), total=len(futures), desc=\"Stacking videos\"):\n",
        "#         err = f.result()\n",
        "#         if err:\n",
        "#             errors.append(err)\n",
        "\n",
        "# print(f\"Completed. {len(errors)} videos failed.\")\n",
        "\n",
        "# if errors:\n",
        "#     print(\"\\nList of all failed videos:\")\n",
        "#     for e in errors:\n",
        "#         print(e)\n",
        "\n",
        "# #!rsync -a --progress \"/content/train_augmented_stacked/\" \"/content/drive/MyDrive/WLASL_300/train_augmented_stacked/\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # if a label folder has under 5 videos, add to ignore list\n",
        "# import os\n",
        "\n",
        "# dataset_dir = \"/content/drive/MyDrive/WLASL_300/train_augmented_stacked\"\n",
        "# ignore_labels = []\n",
        "# for label in os.listdir(dataset_dir):\n",
        "#     label_dir = os.path.join(dataset_dir, label)\n",
        "#     if len(os.listdir(label_dir)) < 5:\n",
        "#         ignore_labels.append(label)\n",
        "\n",
        "# print(f\"Ignored labels: {ignore_labels}\")\n",
        "# # Ignored labels: ['paint', 'order', 'like', 'office', 'match', 'read', 'language', 'old', 'pants', 'make', 'lose', 'rabbit', 'know', 'orange']"
      ],
      "metadata": {
        "id": "PvvyoGFe5H3_",
        "outputId": "19b090eb-e068-409e-e648-6f92a505daf1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ignored labels: ['paint', 'order', 'like', 'office', 'match', 'read', 'language', 'old', 'pants', 'make', 'lose', 'rabbit', 'know', 'orange']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weKr3gO4uyqa"
      },
      "source": [
        "Data loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IlQbqZIxyVLY",
        "outputId": "d4fc9cea-2ddd-4969-bbb2-4bfca04dffa3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Creating dataset\n",
            "There are 185 label folders in /content/drive/MyDrive/WLASL_300/train_augmented_stacked\n",
            "Ignored labels: ['know', 'language', 'like', 'lose', 'make', 'match', 'office', 'old', 'orange', 'order', 'paint', 'pants', 'rabbit', 'read']\n",
            "Found 6737 overlay images across 171 classes\n",
            "Creating dataloaders\n",
            "Classes: ['about', 'accident', 'africa', 'again', 'all', 'always', 'animal', 'apple', 'approve', 'argue'] ...\n",
            "num_classes = 171\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "\n",
        "DATASET_DIR = \"/content/drive/MyDrive/WLASL_300/train_augmented_stacked\"\n",
        "IMG_SIZE = 256\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# ignore label folders with under 5 videos\n",
        "ignored_labels = ['paint', 'order', 'like', 'office', 'match', 'read', 'language', 'old', 'pants', 'make', 'lose', 'rabbit', 'know', 'orange']\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Dataset for loading png overlay files\n",
        "class OverlayDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None, ignore_labels=None):\n",
        "        self.root_dir = Path(root_dir)\n",
        "        self.transform = transform\n",
        "        self.ignore_labels = set(ignore_labels or [])\n",
        "        self.samples = []\n",
        "\n",
        "        if self.ignore_labels:\n",
        "            print(\"Ignored labels:\", sorted(list(self.ignore_labels)))\n",
        "\n",
        "        # Walk through every subfolder and collect overlay PNGs\n",
        "        for class_dir in sorted(self.root_dir.iterdir()):\n",
        "            if not class_dir.is_dir():\n",
        "                continue\n",
        "            label = class_dir.name\n",
        "\n",
        "            # Skip ignored labels\n",
        "            if label in self.ignore_labels:\n",
        "                continue\n",
        "\n",
        "            for img_path in class_dir.glob(\"*.png\"):\n",
        "                self.samples.append((img_path, label))\n",
        "\n",
        "        # Build class index mapping\n",
        "        self.class_names = sorted(list({label for _, label in self.samples}))\n",
        "        self.class_to_idx = {c: i for i, c in enumerate(self.class_names)}\n",
        "\n",
        "        print(f\"Found {len(self.samples)} overlay images across {len(self.class_names)} classes\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, label = self.samples[idx]\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, self.class_to_idx[label]\n",
        "\n",
        "\n",
        "# Image transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
        "])\n",
        "\n",
        "# Load dataset\n",
        "print(\"Creating dataset\")\n",
        "print(f\"There are {len(os.listdir(DATASET_DIR))} label folders in {DATASET_DIR}\")\n",
        "full_ds = OverlayDataset(DATASET_DIR, transform=transform, ignore_labels=ignored_labels)\n",
        "\n",
        "# Train/Val split\n",
        "val_split = 0.2\n",
        "val_size = int(len(full_ds) * val_split)\n",
        "train_size = len(full_ds) - val_size\n",
        "train_ds, val_ds = torch.utils.data.random_split(full_ds, [train_size, val_size])\n",
        "\n",
        "print(\"Creating dataloaders\")\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "class_names = full_ds.class_names\n",
        "num_classes = len(class_names)\n",
        "\n",
        "print(\"Classes:\", class_names[:10], \"...\")\n",
        "print(\"num_classes =\", num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmzS4vdZYfnw"
      },
      "outputs": [],
      "source": [
        "#!pip install torch torchvision torchaudio --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4CO87Py8EjC"
      },
      "source": [
        "Model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "daISlJ6VV5T9",
        "outputId": "4d0854e5-b2cb-4c0b-d864-7b9372f97ad7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 193MB/s]\n",
            "Epoch 1/8: 100%|██████████| 169/169 [15:05<00:00,  5.36s/it, acc=17, loss=3.97]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 complete | Train Acc: 16.96% | Val Acc: 38.23%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/8: 100%|██████████| 169/169 [00:32<00:00,  5.14it/s, acc=51.4, loss=1.2]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 complete | Train Acc: 51.39% | Val Acc: 62.66%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/8: 100%|██████████| 169/169 [00:32<00:00,  5.13it/s, acc=85.8, loss=0.71]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 complete | Train Acc: 85.84% | Val Acc: 80.25%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/8: 100%|██████████| 169/169 [00:33<00:00,  5.07it/s, acc=95.1, loss=0.0116]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 complete | Train Acc: 95.08% | Val Acc: 84.34%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/8: 100%|██████████| 169/169 [00:33<00:00,  5.04it/s, acc=98, loss=0.0504]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 complete | Train Acc: 97.98% | Val Acc: 89.53%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/8: 100%|██████████| 169/169 [00:34<00:00,  4.93it/s, acc=97.5, loss=0.0193]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 complete | Train Acc: 97.51% | Val Acc: 88.64%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/8: 100%|██████████| 169/169 [00:33<00:00,  4.98it/s, acc=97.7, loss=0.0414]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 complete | Train Acc: 97.70% | Val Acc: 89.01%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/8: 100%|██████████| 169/169 [00:34<00:00,  4.85it/s, acc=98.1, loss=0.0905]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 complete | Train Acc: 98.09% | Val Acc: 86.93%\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Just in case cell order runs differently\n",
        "num_classes = len(class_names)\n",
        "\n",
        "# Load pretrained ResNet18\n",
        "model = models.resnet18(weights=\"IMAGENET1K_V1\")\n",
        "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "EPOCHS = 8\n",
        "\n",
        "model.train()\n",
        "for epoch in range(EPOCHS):\n",
        "    total, correct, train_loss = 0, 0, 0\n",
        "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
        "\n",
        "    for images, labels in pbar:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, preds = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += preds.eq(labels).sum().item()\n",
        "        pbar.set_postfix(loss=loss.item(), acc=100.*correct/total)\n",
        "\n",
        "    # ---- Validation ----\n",
        "    model.eval()\n",
        "    val_correct, val_total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, preds = outputs.max(1)\n",
        "            val_total += labels.size(0)\n",
        "            val_correct += preds.eq(labels).sum().item()\n",
        "\n",
        "    val_acc = 100.*val_correct / val_total\n",
        "    print(f\"Epoch {epoch+1} complete | Train Acc: {100.*correct/total:.2f}% | Val Acc: {val_acc:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmDgL-NT8KzL"
      },
      "source": [
        "### Save Model and Labels\n",
        "\n",
        "This cell saves the trained model's state dictionary and the list of class names to files in the `/content/` directory. These files can be used later to load the trained model for inference without retraining."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7-tPJi7x1ziS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c5f50f1-0855-4935-ccd2-a3b3016042cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to: /content/asl_overlay_resnet18.pt\n",
            "Labels saved to: /content/asl_overlay_labels.json\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "MODEL_PATH = \"/content/asl_overlay_resnet18.pt\"\n",
        "LABELS_PATH = \"/content/asl_overlay_labels.json\"\n",
        "\n",
        "torch.save(model.state_dict(), MODEL_PATH)\n",
        "\n",
        "with open(LABELS_PATH, \"w\") as f:\n",
        "    json.dump(class_names, f)\n",
        "\n",
        "print(\"Model saved to:\", MODEL_PATH)\n",
        "print(\"Labels saved to:\", LABELS_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hHaljSI8RKS"
      },
      "source": [
        "Testing model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "eSkbKWdp136D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cab5025b-d59b-407d-cf31-2bdd52e6d918"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping unknown class: kiss\n",
            "Skipping unknown class: knife\n",
            "Ignoring class: know\n",
            "Ignoring class: language\n",
            "Skipping unknown class: last\n",
            "Skipping unknown class: late\n",
            "Skipping unknown class: laugh\n",
            "Skipping unknown class: law\n",
            "Skipping unknown class: learn\n",
            "Skipping unknown class: leave\n",
            "Skipping unknown class: letter\n",
            "Skipping unknown class: light\n",
            "Ignoring class: like\n",
            "Skipping unknown class: live\n",
            "Ignoring class: lose\n",
            "Ignoring class: make\n",
            "Ignoring class: match\n",
            "Skipping unknown class: mean\n",
            "Skipping unknown class: meat\n",
            "Skipping unknown class: meet\n",
            "Skipping unknown class: milk\n",
            "Skipping unknown class: money\n",
            "Skipping unknown class: more\n",
            "Skipping unknown class: most\n",
            "Skipping unknown class: mother\n",
            "Skipping unknown class: movie\n",
            "Skipping unknown class: music\n",
            "Skipping unknown class: name\n",
            "Skipping unknown class: need\n",
            "Skipping unknown class: new\n",
            "Ignoring class: office\n",
            "Ignoring class: old\n",
            "Ignoring class: orange\n",
            "Ignoring class: order\n",
            "Ignoring class: paint\n",
            "Ignoring class: pants\n",
            "Skipping unknown class: party\n",
            "Skipping unknown class: past\n",
            "Skipping unknown class: pencil\n",
            "Skipping unknown class: person\n",
            "Skipping unknown class: pink\n",
            "Skipping unknown class: pizza\n",
            "Skipping unknown class: plan\n",
            "Skipping unknown class: play\n",
            "Skipping unknown class: please\n",
            "Skipping unknown class: police\n",
            "Skipping unknown class: practice\n",
            "Skipping unknown class: president\n",
            "Skipping unknown class: problem\n",
            "Skipping unknown class: pull\n",
            "Skipping unknown class: purple\n",
            "Ignoring class: rabbit\n",
            "Ignoring class: read\n",
            "Skipping unknown class: red\n",
            "Skipping unknown class: remember\n",
            "Skipping unknown class: restaurant\n",
            "Skipping unknown class: ride\n",
            "Skipping unknown class: right\n",
            "Skipping unknown class: room\n",
            "Skipping unknown class: run\n",
            "Skipping unknown class: russia\n",
            "Skipping unknown class: salt\n",
            "Skipping unknown class: same\n",
            "Skipping unknown class: sandwich\n",
            "Skipping unknown class: school\n",
            "Skipping unknown class: secretary\n",
            "Skipping unknown class: share\n",
            "Skipping unknown class: shirt\n",
            "Skipping unknown class: short\n",
            "Skipping unknown class: show\n",
            "Skipping unknown class: sick\n",
            "Skipping unknown class: sign\n",
            "Skipping unknown class: since\n",
            "Skipping unknown class: small\n",
            "Skipping unknown class: snow\n",
            "Skipping unknown class: some\n",
            "Skipping unknown class: son\n",
            "Skipping unknown class: soon\n",
            "Skipping unknown class: south\n",
            "Skipping unknown class: stay\n",
            "Skipping unknown class: student\n",
            "Skipping unknown class: study\n",
            "Skipping unknown class: sunday\n",
            "Skipping unknown class: table\n",
            "Skipping unknown class: take\n",
            "Skipping unknown class: tall\n",
            "Skipping unknown class: tea\n",
            "Skipping unknown class: teach\n",
            "Skipping unknown class: teacher\n",
            "Skipping unknown class: tell\n",
            "Skipping unknown class: test\n",
            "Skipping unknown class: thanksgiving\n",
            "Skipping unknown class: theory\n",
            "Skipping unknown class: thin\n",
            "Skipping unknown class: thursday\n",
            "Skipping unknown class: time\n",
            "Skipping unknown class: tired\n",
            "Skipping unknown class: tomato\n",
            "Skipping unknown class: trade\n",
            "Skipping unknown class: train\n",
            "Skipping unknown class: travel\n",
            "Skipping unknown class: ugly\n",
            "Skipping unknown class: visit\n",
            "Skipping unknown class: wait\n",
            "Skipping unknown class: walk\n",
            "Skipping unknown class: want\n",
            "Skipping unknown class: war\n",
            "Skipping unknown class: water\n",
            "Skipping unknown class: week\n",
            "Skipping unknown class: what\n",
            "Skipping unknown class: where\n",
            "Skipping unknown class: white\n",
            "Skipping unknown class: who\n",
            "Skipping unknown class: why\n",
            "Skipping unknown class: wife\n",
            "Skipping unknown class: window\n",
            "Skipping unknown class: with\n",
            "Skipping unknown class: woman\n",
            "Skipping unknown class: work\n",
            "Skipping unknown class: write\n",
            "Skipping unknown class: wrong\n",
            "Skipping unknown class: year\n",
            "Skipping unknown class: yellow\n",
            "Skipping unknown class: yes\n",
            "Skipping unknown class: yesterday\n",
            "Skipping unknown class: you\n",
            "Skipping unknown class: your\n",
            "Found 299 videos across 167 classes\n",
            "Test Accuracy: 12.04%\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import json\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# def predict_overlay(path):\n",
        "#     img = cv2.imread(path)\n",
        "#     img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "#     img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
        "#     img = torch.tensor(img).permute(2,0,1).float()/255.\n",
        "#     img = transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])(img)\n",
        "#     img = img.unsqueeze(0).to(device)\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         outputs = model(img)\n",
        "#         _, pred = outputs.max(1)\n",
        "#         return class_names[pred.item()]\n",
        "\n",
        "IMG_SIZE = 256\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
        "])\n",
        "\n",
        "def stack_frames_max(frames_bgr, resize=(IMG_SIZE, IMG_SIZE), frame_skip=2):\n",
        "    if not frames_bgr:\n",
        "        return None\n",
        "    frames_bgr = frames_bgr[::max(1, frame_skip)]\n",
        "    acc = None\n",
        "    for f in frames_bgr:\n",
        "        f = cv2.resize(f, resize)\n",
        "        if acc is None:\n",
        "            acc = f.astype(np.float32)\n",
        "        else:\n",
        "            acc = np.maximum(acc, f.astype(np.float32))\n",
        "    return acc.astype(np.uint8)\n",
        "\n",
        "\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, root_dir, class_to_idx, transform=None, ignored_labels=None):\n",
        "        self.root_dir = Path(root_dir)\n",
        "        self.transform = transform\n",
        "        self.class_to_idx = class_to_idx\n",
        "        self.ignored_labels = set(ignored_labels or [])\n",
        "        self.samples = []\n",
        "\n",
        "        for class_dir in sorted(self.root_dir.iterdir()):\n",
        "            if not class_dir.is_dir():\n",
        "                continue\n",
        "            label = class_dir.name\n",
        "\n",
        "            # Skip ignored labels\n",
        "            if label in self.ignored_labels:\n",
        "                print(f\"Ignoring class: {label}\")\n",
        "                continue\n",
        "\n",
        "            # Skip classes not seen during training\n",
        "            if label not in self.class_to_idx:\n",
        "                print(f\"Skipping unknown class: {label}\")\n",
        "                continue\n",
        "\n",
        "            # Collect videos for this class\n",
        "            for video_path in class_dir.glob(\"*.mp4\"):\n",
        "                self.samples.append((video_path, label))\n",
        "\n",
        "        print(f\"Found {len(self.samples)} videos across {len(set(l for _, l in self.samples))} classes\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video_path, label = self.samples[idx]\n",
        "        frames = []\n",
        "        cap = cv2.VideoCapture(str(video_path))\n",
        "        while cap.isOpened():\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            frames.append(frame)\n",
        "        cap.release()\n",
        "\n",
        "        stacked_img = stack_frames_max(frames)\n",
        "        if self.transform:\n",
        "            stacked_img = self.transform(\n",
        "                Image.fromarray(cv2.cvtColor(stacked_img, cv2.COLOR_BGR2RGB))\n",
        "            )\n",
        "\n",
        "        return stacked_img, self.class_to_idx[label]\n",
        "\n",
        "\n",
        "FRAME_SKIP = 2\n",
        "ignored_labels = ['know', 'language', 'like', 'lose', 'make', 'match', 'office', 'old', 'orange', 'order', 'paint', 'pants', 'rabbit', 'read']\n",
        "\n",
        "\n",
        "test_dir = \"/content/drive/MyDrive/WLASL_300/test\"\n",
        "LABELS_PATH = \"/content/asl_overlay_labels.json\"\n",
        "\n",
        "with open(\"/content/asl_overlay_labels.json\", \"r\") as f:\n",
        "    class_names = json.load(f)\n",
        "class_to_idx = {c: i for i, c in enumerate(class_names)}\n",
        "\n",
        "test_dataset = TestDataset(test_dir, class_to_idx, transform=transform, ignored_labels=ignored_labels)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "model = models.resnet18(weights=None)\n",
        "model.fc = nn.Linear(model.fc.in_features, len(class_names))\n",
        "model.load_state_dict(torch.load(\"/content/asl_overlay_resnet18.pt\", map_location=device))\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        outputs = model(images)\n",
        "        _, preds = outputs.max(1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "accuracy = accuracy_score(all_labels, all_preds)\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MffPuW6G8WUf"
      },
      "source": [
        "### Setup Webcam Access (Cell A)\n",
        "\n",
        "This cell injects JavaScript code into the browser to set up webcam access. It creates video and canvas elements, requests user media (webcam), and defines JavaScript functions to capture frames (`getFrame()`) and clean up (`cleanup()`). It then waits for `getFrame()` to be available before proceeding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWQALFEfkDiW"
      },
      "outputs": [],
      "source": [
        "# ==== Cell A: Inject & execute JS in browser, wait until getFrame() is available ====\n",
        "from google.colab import output as colab_output\n",
        "import time, json\n",
        "\n",
        "setup_js = r\"\"\"\n",
        "(async () => {\n",
        "  try {\n",
        "    const video = document.createElement('video');\n",
        "    video.setAttribute('id','asl_cam');\n",
        "    video.style.display = 'block';\n",
        "    video.style.maxWidth = '640px';\n",
        "    video.style.border = '1px solid #ccc';\n",
        "    document.body.appendChild(video);\n",
        "\n",
        "    const stream = await navigator.mediaDevices.getUserMedia({video:{facingMode:\"user\"}, audio:false});\n",
        "    video.srcObject = stream;\n",
        "    await video.play();\n",
        "\n",
        "    const canvas = document.createElement('canvas');\n",
        "    canvas.setAttribute('id','asl_canvas');\n",
        "    const ctx = canvas.getContext('2d');\n",
        "    canvas.width = video.videoWidth || 640;\n",
        "    canvas.height = video.videoHeight || 480;\n",
        "\n",
        "    window.asl_video = video;\n",
        "    window.asl_stream = stream;\n",
        "    window.asl_canvas = canvas;\n",
        "    window.asl_ctx = ctx;\n",
        "\n",
        "    // define getFrame (returns dataURL)\n",
        "    window.getFrame = async function() {\n",
        "      if (!window.asl_video) return null;\n",
        "      window.asl_ctx.drawImage(window.asl_video, 0, 0, window.asl_canvas.width, window.asl_canvas.height);\n",
        "      return window.asl_canvas.toDataURL('image/jpeg', 0.85);\n",
        "    };\n",
        "\n",
        "    // define cleanup\n",
        "    window.cleanup = async function() {\n",
        "      if (window.asl_stream) {\n",
        "        window.asl_stream.getTracks().forEach(t => t.stop());\n",
        "      }\n",
        "      // remove video if present\n",
        "      try { if (window.asl_video && window.asl_video.parentElement) window.asl_video.parentElement.removeChild(window.asl_video); } catch(e){}\n",
        "      delete window.getFrame;\n",
        "      delete window.cleanup;\n",
        "      return true;\n",
        "    };\n",
        "\n",
        "    return {\"ok\": true};\n",
        "  } catch(err) {\n",
        "    return {\"error\": String(err)};\n",
        "  }\n",
        "})();\n",
        "\"\"\"\n",
        "\n",
        "print(\"Injecting webcam JS into browser — you will be prompted to allow camera access.\")\n",
        "res = colab_output.eval_js(setup_js)\n",
        "if isinstance(res, dict) and res.get(\"error\"):\n",
        "    raise RuntimeError(\"JS setup returned error: \" + res[\"error\"])\n",
        "# Poll until getFrame is available (timeout)\n",
        "timeout = 10.0\n",
        "start = time.time()\n",
        "while True:\n",
        "    try:\n",
        "        ok = colab_output.eval_js(\"typeof window.getFrame === 'function'\")\n",
        "    except Exception:\n",
        "        ok = False\n",
        "    if ok:\n",
        "        print(\"getFrame() is available in the browser.\")\n",
        "        break\n",
        "    if time.time() - start > timeout:\n",
        "        raise RuntimeError(\"Timed out waiting for getFrame() to be defined in browser. Try refreshing the page and running this cell again.\")\n",
        "    time.sleep(0.2)\n",
        "\n",
        "print(\"JS webcam helper installed. Now run the main Python loop cell (Cell B).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loRLbcOb8bHh"
      },
      "source": [
        "### Live ASL Recognition Loop (Cell B)\n",
        "\n",
        "This cell contains the main loop for live ASL recognition using the webcam. It continuously captures frames from the browser using the `getFrame()` JavaScript function, processes them to create an overlay image by stacking frames, uses the trained model to predict the ASL sign, and displays the prediction and a running transcript."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAdOWMglkM5n"
      },
      "outputs": [],
      "source": [
        "# ==== Cell B: Live loop — call getFrame(), maintain 1.5s buffer, stack frames, predict, transcript ====\n",
        "import time, base64\n",
        "from collections import deque\n",
        "import json\n",
        "from IPython.display import clear_output, display\n",
        "from google.colab import output as colab_output\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models, transforms\n",
        "\n",
        "# ----------------------- User config (edit if needed) -----------------------\n",
        "MODEL_PATH = \"/content/asl_overlay_resnet18.pt\"\n",
        "LABELS_PATH = \"/content/asl_overlay_labels.json\"\n",
        "\n",
        "IMG_SIZE = 256\n",
        "FRAME_SKIP = 2          # subsample frames when stacking\n",
        "BUFFER_SECONDS = 1.5    # rolling window size in seconds\n",
        "CONF_THRESH = 0.60\n",
        "SHOW_OVERLAY_PREVIEW = True\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "# Load labels + model\n",
        "with open(LABELS_PATH, \"r\") as f:\n",
        "    class_names = json.load(f)\n",
        "num_classes = len(class_names)\n",
        "\n",
        "model = models.resnet18(weights=None)\n",
        "model.fc = torch.nn.Linear(model.fc.in_features, num_classes)\n",
        "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
        "model.eval().to(DEVICE)\n",
        "\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
        "])\n",
        "\n",
        "def stack_frames_max(frames_bgr, resize=(IMG_SIZE, IMG_SIZE), frame_skip=FRAME_SKIP):\n",
        "    if not frames_bgr:\n",
        "        return None\n",
        "    frames_bgr = frames_bgr[::max(1, frame_skip)]\n",
        "    acc = None\n",
        "    for f in frames_bgr:\n",
        "        f = cv2.resize(f, resize)\n",
        "        if acc is None:\n",
        "            acc = f.astype(np.float32)\n",
        "        else:\n",
        "            acc = np.maximum(acc, f.astype(np.float32))\n",
        "    return acc.astype(np.uint8)\n",
        "\n",
        "def predict_stacked_bgr(overlay_bgr):\n",
        "    rgb = cv2.cvtColor(overlay_bgr, cv2.COLOR_BGR2RGB)\n",
        "    rgb = cv2.resize(rgb, (IMG_SIZE, IMG_SIZE))\n",
        "    x = preprocess(rgb).unsqueeze(0).to(DEVICE)\n",
        "    with torch.no_grad():\n",
        "        probs = torch.softmax(model(x), dim=1)[0].cpu().numpy()\n",
        "    idx = int(np.argmax(probs))\n",
        "    return class_names[idx], float(probs[idx])\n",
        "\n",
        "# ----------------------- Live capture loop -----------------------\n",
        "buffer = deque()\n",
        "transcript = []\n",
        "\n",
        "print(\"Live ASL recognition starting. Press Ctrl+C in the cell output to stop.\")\n",
        "try:\n",
        "    while True:\n",
        "        # Request current frame from browser helper\n",
        "        # Use a small retry loop for transient message errors\n",
        "        data_url = None\n",
        "        for _ in range(3):\n",
        "            try:\n",
        "                data_url = colab_output.eval_js(\"getFrame()\")\n",
        "                break\n",
        "            except Exception:\n",
        "                time.sleep(0.05)\n",
        "                continue\n",
        "        if data_url is None:\n",
        "            # If still None, try again\n",
        "            time.sleep(0.01)\n",
        "            continue\n",
        "\n",
        "        # decode data url into BGR frame\n",
        "        header, encoded = data_url.split(\",\", 1)\n",
        "        frame = np.frombuffer(base64.b64decode(encoded), dtype=np.uint8)\n",
        "        frame = cv2.imdecode(frame, cv2.IMREAD_COLOR)  # BGR\n",
        "\n",
        "        now = time.time()\n",
        "        buffer.append((now, frame))\n",
        "\n",
        "        # trim old frames\n",
        "        while buffer and (now - buffer[0][0]) > BUFFER_SECONDS:\n",
        "            buffer.popleft()\n",
        "\n",
        "        # if buffer roughly full -> process\n",
        "        if buffer and (buffer[-1][0] - buffer[0][0]) >= (BUFFER_SECONDS * 0.95):\n",
        "            frames_only = [f for (_, f) in buffer]\n",
        "            overlay = stack_frames_max(frames_only)\n",
        "\n",
        "            if overlay is not None:\n",
        "                label, conf = predict_stacked_bgr(overlay)\n",
        "                if conf >= CONF_THRESH:\n",
        "                    transcript.append(label)\n",
        "                else:\n",
        "                    transcript.append(\"[?]\")\n",
        "\n",
        "                # show stacked preview if desired\n",
        "                if SHOW_OVERLAY_PREVIEW:\n",
        "                    preview = overlay.copy()\n",
        "                    cv2.putText(preview, f\"{label} ({conf:.2f})\", (10, 28),\n",
        "                                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0,255,0), 2)\n",
        "                    rgb = cv2.cvtColor(preview, cv2.COLOR_BGR2RGB)\n",
        "                    clear_output(wait=True)\n",
        "                    display(Image.fromarray(rgb))\n",
        "                else:\n",
        "                    clear_output(wait=True)\n",
        "\n",
        "                # print running transcript\n",
        "                print(\"Live ASL recognition (Stop the cell to end)\\n\")\n",
        "                print(\"Transcript:\\n\", \" \".join(transcript))\n",
        "\n",
        "            # reset buffer to form next 1.5s segment\n",
        "            buffer.clear()\n",
        "\n",
        "        # small sleep\n",
        "        time.sleep(0.01)\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    pass\n",
        "finally:\n",
        "    # request browser cleanup (stop camera)\n",
        "    try:\n",
        "        colab_output.eval_js(\"cleanup()\")\n",
        "    except Exception:\n",
        "        pass\n",
        "    print(\"\\n Stopped. Final transcript:\")\n",
        "    print(\" \".join(transcript))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}