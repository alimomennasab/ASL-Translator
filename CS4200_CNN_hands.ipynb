{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPdX16vUfhdP0m/iajA7QlZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alimomennasab/ASL-Translator/blob/main/CS4200_CNN_hands.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "eq722BFhE_zx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ac93930-d126-4b49-f54b-2359db48b2bb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Preprocessing"
      ],
      "metadata": {
        "id": "UssCtHq7ewlr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mediapipe"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xNoB-4rxJ4wv",
        "outputId": "fda66a5b-acdd-4794-ce34-8f82b575607b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mediapipe\n",
            "  Downloading mediapipe-0.10.21-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (25.4.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (25.9.23)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.7.2)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.7.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from mediapipe) (3.10.0)\n",
            "Collecting numpy<2 (from mediapipe)\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.12/dist-packages (from mediapipe) (4.12.0.88)\n",
            "Collecting protobuf<5,>=4.25.3 (from mediapipe)\n",
            "  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Collecting sounddevice>=0.4.4 (from mediapipe)\n",
            "  Downloading sounddevice-0.5.3-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.2.1)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.12/dist-packages (from sounddevice>=0.4.4->mediapipe) (2.0.0)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe) (0.5.4)\n",
            "INFO: pip is looking at multiple versions of jax to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting jax (from mediapipe)\n",
            "  Downloading jax-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting jaxlib (from mediapipe)\n",
            "  Downloading jaxlib-0.8.1-cp312-cp312-manylinux_2_27_x86_64.whl.metadata (1.3 kB)\n",
            "Collecting jax (from mediapipe)\n",
            "  Downloading jax-0.8.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting jaxlib (from mediapipe)\n",
            "  Downloading jaxlib-0.8.0-cp312-cp312-manylinux_2_27_x86_64.whl.metadata (1.3 kB)\n",
            "Collecting jax (from mediapipe)\n",
            "  Downloading jax-0.7.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting jaxlib (from mediapipe)\n",
            "  Downloading jaxlib-0.7.1-cp312-cp312-manylinux_2_27_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.12 in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe) (1.16.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (2.9.0.post0)\n",
            "INFO: pip is looking at multiple versions of opencv-contrib-python to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting opencv-contrib-python (from mediapipe)\n",
            "  Downloading opencv_contrib_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.23)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.17.0)\n",
            "Downloading mediapipe-0.10.21-cp312-cp312-manylinux_2_28_x86_64.whl (35.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.6/35.6 MB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m67.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sounddevice-0.5.3-py3-none-any.whl (32 kB)\n",
            "Downloading jax-0.7.1-py3-none-any.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jaxlib-0.7.1-cp312-cp312-manylinux_2_27_x86_64.whl (81.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.2/81.2 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencv_contrib_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (69.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: protobuf, numpy, sounddevice, opencv-contrib-python, jaxlib, jax, mediapipe\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.5\n",
            "    Uninstalling protobuf-5.29.5:\n",
            "      Successfully uninstalled protobuf-5.29.5\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: opencv-contrib-python\n",
            "    Found existing installation: opencv-contrib-python 4.12.0.88\n",
            "    Uninstalling opencv-contrib-python-4.12.0.88:\n",
            "      Successfully uninstalled opencv-contrib-python-4.12.0.88\n",
            "  Attempting uninstall: jaxlib\n",
            "    Found existing installation: jaxlib 0.7.2\n",
            "    Uninstalling jaxlib-0.7.2:\n",
            "      Successfully uninstalled jaxlib-0.7.2\n",
            "  Attempting uninstall: jax\n",
            "    Found existing installation: jax 0.7.2\n",
            "    Uninstalling jax-0.7.2:\n",
            "      Successfully uninstalled jax-0.7.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 4.25.8 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed jax-0.7.1 jaxlib-0.7.1 mediapipe-0.10.21 numpy-1.26.4 opencv-contrib-python-4.11.0.86 protobuf-4.25.8 sounddevice-0.5.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "numpy"
                ]
              },
              "id": "9ede0f6da56f41e4a630bbc1d573a110"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "KBBsR1HkCqnH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1385263f-f2f5-46a4-ff21-9d6b83f030a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|▎         | 3/100 [00:19<10:26,  6.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[WARNING] Empty or unreadable video: /content/drive/MyDrive/WLASL/WLASL100_val/orange/40118.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [12:05<00:00,  7.25s/it]\n",
            "100%|██████████| 86/86 [06:43<00:00,  4.70s/it]\n"
          ]
        }
      ],
      "source": [
        "# Data preprocessing: extract right and left hands from both videos\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import mediapipe as mp\n",
        "from tqdm import tqdm\n",
        "\n",
        "CROP_SIZE = 112\n",
        "TARGET_FRAMES = 64\n",
        "\n",
        "mp_hands = mp.solutions.hands\n",
        "hands = mp_hands.Hands(\n",
        "    static_image_mode=False,\n",
        "    max_num_hands=2,\n",
        "    min_detection_confidence=0.5,\n",
        "    min_tracking_confidence=0.5\n",
        ")\n",
        "\n",
        "def extract_hand_boxes(frame, results):\n",
        "    h, w = frame.shape[:2]\n",
        "    left_box, right_box = None, None\n",
        "\n",
        "    if results.multi_hand_landmarks and results.multi_handedness:\n",
        "        for lm, handness in zip(results.multi_hand_landmarks, results.multi_handedness):\n",
        "            label = handness.classification[0].label.lower()\n",
        "\n",
        "            xs = [p.x for p in lm.landmark]\n",
        "            ys = [p.y for p in lm.landmark]\n",
        "\n",
        "            x1 = int(w * min(xs))\n",
        "            x2 = int(w * max(xs))\n",
        "            y1 = int(h * min(ys))\n",
        "            y2 = int(h * max(ys))\n",
        "\n",
        "            # bounding box padding\n",
        "            pad = 20\n",
        "            x1 = max(0, x1 - pad)\n",
        "            y1 = max(0, y1 - pad)\n",
        "            x2 = min(w, x2 + pad)\n",
        "            y2 = min(h, y2 + pad)\n",
        "\n",
        "            box = (x1, y1, x2, y2)\n",
        "\n",
        "            if label == \"left\":\n",
        "                left_box = box\n",
        "            else:\n",
        "                right_box = box\n",
        "\n",
        "    return left_box, right_box\n",
        "\n",
        "\n",
        "def extract_crops_from_video(path):\n",
        "    frames = []\n",
        "    cap = cv2.VideoCapture(path)\n",
        "\n",
        "    while True:\n",
        "        ret, f = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frames.append(f)\n",
        "    cap.release()\n",
        "\n",
        "    # 64 frames in each vid\n",
        "    if len(frames) == 0:\n",
        "        print(f\"[WARNING] Empty or unreadable video: {path}\")\n",
        "        return None, None\n",
        "    elif len(frames) > TARGET_FRAMES:\n",
        "        idxs = np.linspace(0, len(frames)-1, TARGET_FRAMES).astype(int)\n",
        "        frames = [frames[i] for i in idxs]\n",
        "    elif len(frames) < TARGET_FRAMES:\n",
        "        frames = frames + [frames[-1]]*(TARGET_FRAMES - len(frames))\n",
        "\n",
        "    left_crops = []\n",
        "    right_crops = []\n",
        "\n",
        "    for f in frames:\n",
        "        rgb = cv2.cvtColor(f, cv2.COLOR_BGR2RGB)\n",
        "        results = hands.process(rgb)\n",
        "\n",
        "        lbox, rbox = extract_hand_boxes(f, results)\n",
        "\n",
        "        # left\n",
        "        if lbox is None:\n",
        "            left_c = np.zeros((CROP_SIZE, CROP_SIZE, 3), dtype=np.uint8)\n",
        "        else:\n",
        "            x1,y1,x2,y2 = lbox\n",
        "            left = f[y1:y2, x1:x2]\n",
        "            left_c = cv2.resize(left, (CROP_SIZE, CROP_SIZE))\n",
        "\n",
        "        # right\n",
        "        if rbox is None:\n",
        "            right_c = np.zeros((CROP_SIZE, CROP_SIZE, 3), dtype=np.uint8)\n",
        "        else:\n",
        "            x1,y1,x2,y2 = rbox\n",
        "            right = f[y1:y2, x1:x2]\n",
        "            right_c = cv2.resize(right, (CROP_SIZE, CROP_SIZE))\n",
        "\n",
        "        left_crops.append(left_c)\n",
        "        right_crops.append(right_c)\n",
        "\n",
        "    return np.stack(left_crops), np.stack(right_crops)\n",
        "\n",
        "\n",
        "def process_hands(video_root, output_root):\n",
        "    for label in tqdm(os.listdir(video_root)):\n",
        "        ldir = os.path.join(video_root, label)\n",
        "        if not os.path.isdir(ldir):\n",
        "            continue\n",
        "\n",
        "        out_label_dir = os.path.join(output_root, label)\n",
        "        os.makedirs(out_label_dir, exist_ok=True)\n",
        "\n",
        "        for f in os.listdir(ldir):\n",
        "            if not f.endswith(\".mp4\"):\n",
        "                continue\n",
        "\n",
        "            video_path = os.path.join(ldir, f)\n",
        "            base = f.replace(\".mp4\",\"\")\n",
        "\n",
        "            left, right = extract_crops_from_video(video_path)\n",
        "\n",
        "            if left is None:\n",
        "                print(f\"Video unreadable: {video_path}\")\n",
        "                continue\n",
        "\n",
        "            np.save(os.path.join(out_label_dir, base+\"_left.npy\"), left)\n",
        "            np.save(os.path.join(out_label_dir, base+\"_right.npy\"), right)\n",
        "\n",
        "# Processing\n",
        "HAND_TRAIN_INPUT = \"/content/drive/MyDrive/WLASL/WLASL100_train\"\n",
        "HAND_VAL_INPUT = \"/content/drive/MyDrive/WLASL/WLASL100_val\"\n",
        "HAND_TEST_INPUT = \"/content/drive/MyDrive/WLASL/WLASL100_test\"\n",
        "\n",
        "HAND_TRAIN_OUTPUT = \"/content/drive/MyDrive/WLASL/HAND_TRAIN\"\n",
        "HAND_VAL_OUTPUT = \"/content/drive/MyDrive/WLASL/HAND_VAL\"\n",
        "HAND_TEST_OUTPUT = \"/content/drive/MyDrive/WLASL/HAND_TEST\"\n",
        "os.makedirs(HAND_TRAIN_OUTPUT, exist_ok=True)\n",
        "os.makedirs(HAND_VAL_OUTPUT, exist_ok=True)\n",
        "os.makedirs(HAND_TEST_OUTPUT, exist_ok=True)\n",
        "\n",
        "#process_hands(HAND_TRAIN_INPUT, HAND_TRAIN_OUTPUT)\n",
        "process_hands(HAND_VAL_INPUT, HAND_VAL_OUTPUT)\n",
        "process_hands(HAND_TEST_INPUT, HAND_TEST_OUTPUT)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Data Loading"
      ],
      "metadata": {
        "id": "Mo1INliseuYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "CROP_SIZE = 112\n",
        "SEQ_LEN = 64\n",
        "\n",
        "img_tf = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std =[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "class HandCropDataset(Dataset):\n",
        "    def __init__(self, root_dir, class_map=None, train=True):\n",
        "        self.root_dir = root_dir\n",
        "        self.samples = []\n",
        "        self.train = train\n",
        "\n",
        "        # build class map\n",
        "        if class_map is None:\n",
        "            labels = sorted([\n",
        "                d for d in os.listdir(root_dir)\n",
        "                if os.path.isdir(os.path.join(root_dir, d))\n",
        "            ])\n",
        "            self.class_to_idx = {lbl:i for i,lbl in enumerate(labels)}\n",
        "        else:\n",
        "            self.class_to_idx = class_map[\"class_to_idx\"]\n",
        "\n",
        "        self.idx_to_class = list(self.class_to_idx.keys())\n",
        "\n",
        "        # append samples\n",
        "        for lbl in self.class_to_idx:\n",
        "            lbl_dir = os.path.join(root_dir, lbl)\n",
        "            idx = self.class_to_idx[lbl]\n",
        "\n",
        "            for f in os.listdir(lbl_dir):\n",
        "                if f.endswith(\"_left.npy\"):\n",
        "                    base = f.replace(\"_left.npy\", \"\")\n",
        "                    left_path = os.path.join(lbl_dir, f)\n",
        "                    right_path = os.path.join(lbl_dir, base + \"_right.npy\")\n",
        "\n",
        "                    if os.path.exists(right_path):\n",
        "                        self.samples.append((left_path, right_path, idx))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        left_path, right_path, label = self.samples[idx]\n",
        "\n",
        "        left = np.load(left_path) # (num_frames, H, W, 3)\n",
        "        right = np.load(right_path)\n",
        "\n",
        "        left_ts  = torch.stack([img_tf(Image.fromarray(f)) for f in left])\n",
        "        right_ts = torch.stack([img_tf(Image.fromarray(f)) for f in right])\n",
        "\n",
        "        return left_ts, right_ts, torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "\n",
        "TRAIN_DIR = \"/content/drive/MyDrive/WLASL/HAND_TRAIN\"\n",
        "VAL_DIR = \"/content/drive/MyDrive/WLASL/HAND_VAL\"\n",
        "\n",
        "train_ds = HandCropDataset(TRAIN_DIR)\n",
        "val_ds = HandCropDataset(VAL_DIR, class_map={'class_to_idx': train_ds.class_to_idx})\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=8, shuffle=True, num_workers=2)\n",
        "val_loader = torch.utils.data.DataLoader(val_ds, batch_size=8, shuffle=False, num_workers=2)\n",
        "print(f\"Train samples: {len(train_ds)}\")\n",
        "print(f\"Val samples: {len(val_ds)}\")\n",
        "\n",
        "num_classes = len(train_ds.idx_to_class)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "id": "ItK2PR2He4ii",
        "outputId": "839797ff-2047-42ac-d324-f463f778a104"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/WLASL/HAND_VAL/orange'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4019537899.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0mtrain_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHandCropDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m \u001b[0mval_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHandCropDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVAL_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'class_to_idx'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_to_idx\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4019537899.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root_dir, class_map, train)\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_to_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlbl\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlbl_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_left.npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                     \u001b[0mbase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_left.npy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/WLASL/HAND_VAL/orange'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cc73af3"
      },
      "source": [
        "# Data preprocessing: extract right and left hands from both videos\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import mediapipe as mp\n",
        "from tqdm import tqdm\n",
        "\n",
        "CROP_SIZE = 112\n",
        "TARGET_FRAMES = 64\n",
        "\n",
        "mp_hands = mp.solutions.hands\n",
        "hands = mp_hands.Hands(\n",
        "    static_image_mode=False,\n",
        "    max_num_hands=2,\n",
        "    min_detection_confidence=0.5,\n",
        "    min_tracking_confidence=0.5\n",
        ")\n",
        "\n",
        "def extract_hand_boxes(frame, results):\n",
        "    h, w = frame.shape[:2]\n",
        "    left_box, right_box = None, None\n",
        "\n",
        "    if results.multi_hand_landmarks and results.multi_handedness:\n",
        "        for lm, handness in zip(results.multi_hand_landmarks, results.multi_handedness):\n",
        "            label = handness.classification[0].label.lower()\n",
        "\n",
        "            xs = [p.x for p in lm.landmark]\n",
        "            ys = [p.y for p in lm.landmark]\n",
        "\n",
        "            x1 = int(w * min(xs))\n",
        "            x2 = int(w * max(xs))\n",
        "            y1 = int(h * min(ys))\n",
        "            y2 = int(h * max(ys))\n",
        "\n",
        "            # bounding box padding\n",
        "            pad = 20\n",
        "            x1 = max(0, x1 - pad)\n",
        "            y1 = max(0, y1 - pad)\n",
        "            x2 = min(w, x2 + pad)\n",
        "            y2 = min(h, y2 + pad)\n",
        "\n",
        "            box = (x1, y1, x2, y2)\n",
        "\n",
        "            if label == \"left\":\n",
        "                left_box = box\n",
        "            else:\n",
        "                right_box = box\n",
        "\n",
        "    return left_box, right_box\n",
        "\n",
        "\n",
        "def extract_crops_from_video(path):\n",
        "    frames = []\n",
        "    cap = cv2.VideoCapture(path)\n",
        "\n",
        "    while True:\n",
        "        ret, f = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frames.append(f)\n",
        "    cap.release()\n",
        "\n",
        "    # case: no frames were read from the video\n",
        "    if not frames:\n",
        "        return (np.zeros((TARGET_FRAMES, CROP_SIZE, CROP_SIZE, 3), dtype=np.uint8),\n",
        "                np.zeros((TARGET_FRAMES, CROP_SIZE, CROP_SIZE, 3), dtype=np.uint8))\n",
        "\n",
        "    # 64 frames in each vid\n",
        "    if len(frames) > TARGET_FRAMES:\n",
        "        idxs = np.linspace(0, len(frames)-1, TARGET_FRAMES).astype(int)\n",
        "        frames = [frames[i] for i in idxs]\n",
        "    elif len(frames) < TARGET_FRAMES:\n",
        "        frames = frames + [frames[-1]]*(TARGET_FRAMES - len(frames))\n",
        "\n",
        "    left_crops = []\n",
        "    right_crops = []\n",
        "\n",
        "    for f in frames:\n",
        "        rgb = cv2.cvtColor(f, cv2.COLOR_BGR2RGB)\n",
        "        results = hands.process(rgb)\n",
        "\n",
        "        lbox, rbox = extract_hand_boxes(f, results)\n",
        "\n",
        "        # left\n",
        "        if lbox is None:\n",
        "            left_c = np.zeros((CROP_SIZE, CROP_SIZE, 3), dtype=np.uint8)\n",
        "        else:\n",
        "            x1,y1,x2,y2 = lbox\n",
        "            left = f[y1:y2, x1:x2]\n",
        "            left_c = cv2.resize(left, (CROP_SIZE, CROP_SIZE))\n",
        "\n",
        "        # right\n",
        "        if rbox is None:\n",
        "            right_c = np.zeros((CROP_SIZE, CROP_SIZE, 3), dtype=np.uint8)\n",
        "        else:\n",
        "            x1,y1,x2,y2 = rbox\n",
        "            right = f[y1:y2, x1:x2]\n",
        "            right_c = cv2.resize(right, (CROP_SIZE, CROP_SIZE))\n",
        "\n",
        "        left_crops.append(left_c)\n",
        "        right_crops.append(right_c)\n",
        "\n",
        "    return np.stack(left_crops), np.stack(right_crops)\n",
        "\n",
        "\n",
        "def process_hands(video_root, output_root):\n",
        "    for label in tqdm(os.listdir(video_root)):\n",
        "        ldir = os.path.join(video_root, label)\n",
        "        if not os.path.isdir(ldir):\n",
        "            continue\n",
        "\n",
        "        out_label_dir = os.path.join(output_root, label)\n",
        "        os.makedirs(out_label_dir, exist_ok=True)\n",
        "\n",
        "        for f in os.listdir(ldir):\n",
        "            if not f.endswith(\".mp4\"):\n",
        "                continue\n",
        "\n",
        "            video_path = os.path.join(ldir, f)\n",
        "            base = f.replace(\".mp4\",\"\")\n",
        "\n",
        "            left, right = extract_crops_from_video(video_path)\n",
        "\n",
        "            # Only save if crops are not empty (i.e., video was readable)\n",
        "            if not np.all(left == 0) or not np.all(right == 0):\n",
        "                np.save(os.path.join(out_label_dir, base+\"_left.npy\"), left)\n",
        "                np.save(os.path.join(out_label_dir, base+\"_right.npy\"), right)\n",
        "            else:\n",
        "                print(f\"[INFO] Skipping {video_path} due to empty or unreadable content.\")\n",
        "\n",
        "# Processing\n",
        "HAND_TRAIN_INPUT = \"/content/drive/MyDrive/WLASL/WLASL100_train\"\n",
        "HAND_VAL_INPUT = \"/content/drive/MyDrive/WLASL/WLASL100_val\"\n",
        "HAND_TEST_INPUT = \"/content/drive/MyDrive/WLASL/WLASL100_test\"\n",
        "\n",
        "HAND_TRAIN_OUTPUT = \"/content/drive/MyDrive/WLASL/HAND_TRAIN\"\n",
        "HAND_VAL_OUTPUT = \"/content/drive/MyDrive/WLASL/HAND_VAL\"\n",
        "HAND_TEST_OUTPUT = \"/content/drive/MyDrive/WLASL/HAND_TEST\"\n",
        "os.makedirs(HAND_TRAIN_OUTPUT, exist_ok=True)\n",
        "os.makedirs(HAND_VAL_OUTPUT, exist_ok=True)\n",
        "os.makedirs(HAND_TEST_OUTPUT, exist_ok=True)\n",
        "\n",
        "process_hands(HAND_TRAIN_INPUT, HAND_TRAIN_OUTPUT)\n",
        "process_hands(HAND_VAL_INPUT, HAND_VAL_OUTPUT)\n",
        "process_hands(HAND_TEST_INPUT, HAND_TEST_OUTPUT)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7a96ad9",
        "outputId": "b209d5ae-7a85-478b-9bdd-b361f6775924"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "CROP_SIZE = 112\n",
        "SEQ_LEN = 64\n",
        "\n",
        "img_tf = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std =[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "class HandCropDataset(Dataset):\n",
        "    def __init__(self, root_dir, class_map=None, train=True):\n",
        "        self.root_dir = root_dir\n",
        "        self.samples = []\n",
        "        self.train = train\n",
        "\n",
        "        # build class map\n",
        "        if class_map is None:\n",
        "            labels = sorted([\n",
        "                d for d in os.listdir(root_dir)\n",
        "                if os.path.isdir(os.path.join(root_dir, d))\n",
        "            ])\n",
        "            self.class_to_idx = {lbl:i for i,lbl in enumerate(labels)}\n",
        "        else:\n",
        "            self.class_to_idx = class_map[\"class_to_idx\"]\n",
        "\n",
        "        self.idx_to_class = list(self.class_to_idx.keys())\n",
        "\n",
        "        # append samples\n",
        "        for lbl in self.class_to_idx:\n",
        "            lbl_dir = os.path.join(root_dir, lbl)\n",
        "            idx = self.class_to_idx[lbl]\n",
        "\n",
        "            # Check if the label directory exists before listing its contents\n",
        "            if not os.path.isdir(lbl_dir):\n",
        "                print(f\"[WARNING] Label directory not found: {lbl_dir}. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            for f in os.listdir(lbl_dir):\n",
        "                if f.endswith(\"_left.npy\"):\n",
        "                    base = f.replace(\"_left.npy\", \"\")\n",
        "                    left_path = os.path.join(lbl_dir, f)\n",
        "                    right_path = os.path.join(lbl_dir, base + \"_right.npy\")\n",
        "\n",
        "                    if os.path.exists(right_path):\n",
        "                        self.samples.append((left_path, right_path, idx))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        left_path, right_path, label = self.samples[idx]\n",
        "\n",
        "        left = np.load(left_path) # (num_frames, H, W, 3)\n",
        "        right = np.load(right_path)\n",
        "\n",
        "        left_ts  = torch.stack([img_tf(Image.fromarray(f)) for f in left])\n",
        "        right_ts = torch.stack([img_tf(Image.fromarray(f)) for f in right])\n",
        "\n",
        "        return left_ts, right_ts, torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "\n",
        "TRAIN_DIR = \"/content/drive/MyDrive/WLASL/HAND_TRAIN\"\n",
        "VAL_DIR = \"/content/drive/MyDrive/WLASL/HAND_VAL\"\n",
        "\n",
        "train_ds = HandCropDataset(TRAIN_DIR)\n",
        "val_ds = HandCropDataset(VAL_DIR, class_map={'class_to_idx': train_ds.class_to_idx})\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=8, shuffle=True, num_workers=2)\n",
        "val_loader = torch.utils.data.DataLoader(val_ds, batch_size=8, shuffle=False, num_workers=2)\n",
        "print(f\"Train samples: {len(train_ds)}\")\n",
        "print(f\"Val samples: {len(val_ds)}\")\n",
        "\n",
        "num_classes = len(train_ds.idx_to_class)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[WARNING] Label directory not found: /content/drive/MyDrive/WLASL/HAND_VAL/orange. Skipping.\n",
            "Train samples: 470\n",
            "Val samples: 197\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Definition"
      ],
      "metadata": {
        "id": "0I3BQfmvjhVn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "from torchvision import models\n",
        "\n",
        "class HandCNN(nn.Module):\n",
        "    def __init__(self, num_classes, feat_dim=256):\n",
        "        super().__init__()\n",
        "\n",
        "        m = models.mobilenet_v3_small(weights=models.MobileNet_V3_Small_Weights.IMAGENET1K_V1)\n",
        "        self.cnn = m.features # feature extraction layers\n",
        "        self.pool = nn.AdaptiveAvgPool2d(1) # adaptive average pooling to get a fixed size feature vector (1 x 1)\n",
        "\n",
        "        in_feats = m.classifier[0].in_features\n",
        "\n",
        "        self.fc1 = nn.Linear(in_feats, feat_dim)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "        self.clf = nn.Linear(feat_dim * 2, num_classes) # feat_dim * 2 because left and right hand features are concatenated\n",
        "\n",
        "    def forward(self, left, right):\n",
        "        B, T, C, H, W = left.shape\n",
        "\n",
        "        # combine batch size and time steps to treat each frame as an independent image\n",
        "        L = left.view(B*T, C, H, W)\n",
        "        R = right.view(B*T, C, H, W)\n",
        "\n",
        "        # pass each hand's frames through the feature extractor\n",
        "        Lf = self.pool(self.cnn(L)).squeeze() # (B*T, CNN_features_dim, 1, 1) -> (B*T, CNN_features_dim)\n",
        "        Rf = self.pool(self.cnn(R)).squeeze() # (B*T, CNN_features_dim, 1, 1) -> (B*T, CNN_features_dim)\n",
        "        Lf = self.dropout(self.relu(self.fc1(Lf)))\n",
        "        Rf = self.dropout(self.relu(self.fc1(Rf)))\n",
        "\n",
        "        # reshape features back to include the time dimension, average features over time\n",
        "        Lf = Lf.view(B, T, -1).mean(dim=1) # (B, T, feat_dim) -> (B, feat_dim)\n",
        "        Rf = Rf.view(B, T, -1).mean(dim=1)\n",
        "\n",
        "        # concatenate the averaged left and right hand features\n",
        "        fused = torch.cat([Lf, Rf], dim=1) # (B, feat_dim * 2)\n",
        "\n",
        "        # classify with fused features\n",
        "        return self.clf(fused)"
      ],
      "metadata": {
        "id": "vbD9G-D7jibo"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}