{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alimomennasab/ASL-Translator/blob/main/CS4200_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bGBv3Zg_CvdV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1abb9acd-c3fb-4c06-8cac-dd29bbca975e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Preprocessing"
      ],
      "metadata": {
        "id": "XHeb0AUBKToG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# extract keypoints from videos\n",
        "\n",
        "import tqdm\n",
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "# take the results of holistic.process on a frame and extract keypoints\n",
        "def extract_keypoint_vector(results):\n",
        "    keypoints = []\n",
        "\n",
        "    # Pose\n",
        "    if results.pose_landmarks:\n",
        "        for lm in results.pose_landmarks.landmark:\n",
        "            keypoints.extend([lm.x, lm.y, lm.z])\n",
        "    else:\n",
        "        keypoints.extend([0.0] * 33 * 3)\n",
        "\n",
        "    # Left hand\n",
        "    if results.left_hand_landmarks:\n",
        "        for lm in results.left_hand_landmarks.landmark:\n",
        "            keypoints.extend([lm.x, lm.y, lm.z])\n",
        "    else:\n",
        "        keypoints.extend([0.0] * 21 * 3)\n",
        "\n",
        "    # Right hand\n",
        "    if results.right_hand_landmarks:\n",
        "        for lm in results.right_hand_landmarks.landmark:\n",
        "            keypoints.extend([lm.x, lm.y, lm.z])\n",
        "    else:\n",
        "        keypoints.extend([0.0] * 21 * 3)\n",
        "\n",
        "    return np.array(keypoints, dtype=np.float32) # (75*3,)\n",
        "\n",
        "\n",
        "# go through each frame of a video and perform keypoint extraction\n",
        "def extract_video_keypoints(vid_path, holistic):\n",
        "    # convert video to frames\n",
        "    cap = cv2.VideoCapture(os.path.join(vid_path))\n",
        "    frames = []\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frames.append(frame)\n",
        "    cap.release()\n",
        "\n",
        "    kp_seq = []\n",
        "\n",
        "    for frame in frames:\n",
        "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        results = holistic.process(rgb)\n",
        "        kp_vec = extract_keypoint_vector(results)\n",
        "        kp_seq.append(kp_vec)\n",
        "\n",
        "    return np.stack(kp_seq, axis=0)\n",
        "\n",
        "\n",
        "# Processing entire dataset\n",
        "def process_dataset(dataset_dir, output_dir):\n",
        "    mp_holistic = mp.solutions.holistic\n",
        "    with mp_holistic.Holistic(\n",
        "        static_image_mode=False,\n",
        "        model_complexity=1,\n",
        "        enable_segmentation=False,\n",
        "        refine_face_landmarks=False\n",
        "    ) as holistic:\n",
        "\n",
        "      for label in tqdm.tqdm(os.listdir(dataset_dir)):\n",
        "          label_dir = os.path.join(dataset_dir, label)\n",
        "          output_label_dir = os.path.join(output_dir, label)\n",
        "          os.makedirs(output_label_dir, exist_ok=True)\n",
        "\n",
        "          for video in os.listdir(label_dir):\n",
        "              video_path = os.path.join(label_dir, video)\n",
        "              base = os.path.splitext(video)[0]\n",
        "              output_video_path = os.path.join(output_label_dir, base + \".npy\")\n",
        "\n",
        "              kp_seq = extract_video_keypoints(video_path, holistic)\n",
        "              np.save(output_video_path, kp_seq)\n",
        "\n",
        "# Main processing\n",
        "target_frames = 64\n",
        "\n",
        "TRAIN_DATASET_DIR = f\"/content/drive/MyDrive/WLASL/WLASL100_train_augmented_{target_frames}frames\"\n",
        "VAL_DATASET_DIR = f'/content/drive/MyDrive/WLASL/WLASL100_val_{target_frames}frames'\n",
        "TEST_DATASET_DIR = f'/content/drive/MyDrive/WLASL/WLASL100_test_{target_frames}frames'\n",
        "\n",
        "KP_TRAIN_DATASET_DIR = f\"/content/drive/MyDrive/WLASL/WLASL100_KP_train_augmented_{target_frames}\"\n",
        "KP_VAL_DATASET_DIR = f'/content/drive/MyDrive/WLASL/WLASL100_KP_val_{target_frames}frames'\n",
        "KP_TEST_DATASET_DIR = f'/content/drive/MyDrive/WLASL/WLASL100_KP_test_{target_frames}frames'\n",
        "os.makedirs(KP_TRAIN_DATASET_DIR, exist_ok=True)\n",
        "os.makedirs(KP_VAL_DATASET_DIR, exist_ok=True)\n",
        "os.makedirs(KP_TEST_DATASET_DIR, exist_ok=True)\n",
        "\n",
        "#process_dataset(dataset_dir=TRAIN_DATASET_DIR, output_dir=KP_TRAIN_DATASET_DIR)\n",
        "#process_dataset(dataset_dir=VAL_DATASET_DIR, output_dir=KP_VAL_DATASET_DIR)\n",
        "process_dataset(dataset_dir=TEST_DATASET_DIR, output_dir= KP_TEST_DATASET_DIR)"
      ],
      "metadata": {
        "id": "kvmV0EmWKUu6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b2686ac-a418-4272-ba8a-9480c754e0fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 86/86 [07:57<00:00,  5.55s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Loading"
      ],
      "metadata": {
        "id": "AsozKp16DeOu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mediapipe --quiet\n",
        "\n",
        "!pip uninstall numpy -y --quiet\n",
        "!pip install numpy --quiet\n",
        "!pip uninstall mediapipe -y --quiet\n",
        "!pip install mediapipe --quiet\n",
        "!pip uninstall tensorflow -y --quiet\n",
        "!pip install tensorflow --quiet\n",
        "!pip uninstall jax -y --quiet\n",
        "!pip install jax --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWJF41qA5bLt",
        "outputId": "9e6684e1-af47-49ff-e133-575b145b88b1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m85.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "mediapipe 0.10.21 requires numpy<2, but you have numpy 2.3.5 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.5 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.5 which is incompatible.\n",
            "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.3.5 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.5 which is incompatible.\n",
            "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m620.7/620.7 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.2/323.2 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "mediapipe 0.10.21 requires protobuf<5,>=4.25.3, but you have protobuf 6.33.1 which is incompatible.\n",
            "tf-keras 2.19.0 requires tensorflow<2.20,>=2.19, but you have tensorflow 2.20.0 which is incompatible.\n",
            "tensorflow-text 2.19.0 requires tensorflow<2.20,>=2.19.0, but you have tensorflow 2.20.0 which is incompatible.\n",
            "tensorflow-decision-forests 1.12.0 requires tensorflow==2.19.0, but you have tensorflow 2.20.0 which is incompatible.\n",
            "google-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.33.1 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 6.33.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.3/80.3 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "mediapipe 0.10.21 requires numpy<2, but you have numpy 2.3.5 which is incompatible.\n",
            "mediapipe 0.10.21 requires protobuf<5,>=4.25.3, but you have protobuf 6.33.1 which is incompatible.\n",
            "tf-keras 2.19.0 requires tensorflow<2.20,>=2.19, but you have tensorflow 2.20.0 which is incompatible.\n",
            "tensorflow-text 2.19.0 requires tensorflow<2.20,>=2.19.0, but you have tensorflow 2.20.0 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.5 which is incompatible.\n",
            "tensorflow-decision-forests 1.12.0 requires tensorflow==2.19.0, but you have tensorflow 2.20.0 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.5 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import cv2\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "\n",
        "target_frames = 64\n",
        "TRAIN_DATASET_DIR = f\"/content/drive/MyDrive/WLASL/WLASL100_KP_train_augmented_{target_frames}\"\n",
        "VAL_DATASET_DIR = f'/content/drive/MyDrive/WLASL/WLASL100_KP_val_{target_frames}frames'\n",
        "BATCH_SIZE = 4\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "    print(\"Using GPU\")\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "    print(\"Using CPU\")\n",
        "\n",
        "\n",
        "class KeypointDataset(Dataset):\n",
        "    def __init__(self, root_dir, class_map=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.samples = [] # (path, label_idx) tuples\n",
        "\n",
        "        if class_map is None:\n",
        "            self.class_to_idx = {}\n",
        "            self.idx_to_class = []\n",
        "            label_folders = sorted([d for d in os.listdir(root_dir)\n",
        "                                    if os.path.isdir(os.path.join(root_dir, d))])\n",
        "            for i, label_name in enumerate(label_folders):\n",
        "                self.class_to_idx[label_name] = i\n",
        "                self.idx_to_class.append(label_name)\n",
        "        else:\n",
        "            self.class_to_idx = class_map[\"class_to_idx\"]\n",
        "            self.idx_to_class = class_map[\"idx_to_class\"]\n",
        "\n",
        "\n",
        "        for label in os.listdir(root_dir):\n",
        "            if label not in self.class_to_idx:\n",
        "                continue\n",
        "            label_path = os.path.join(root_dir, label)\n",
        "            if not os.path.isdir(label_path):\n",
        "                continue\n",
        "\n",
        "            label_idx = self.class_to_idx[label]\n",
        "            for keypoint_arr in os.listdir(label_path):\n",
        "                path = os.path.join(label_path, keypoint_arr)\n",
        "                self.samples.append((path, label_idx))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path, label = self.samples[idx]\n",
        "        seq = np.load(path)\n",
        "\n",
        "        # normalize sequence\n",
        "        mean = seq.mean(axis=0, keepdims=True)\n",
        "        std = seq.std(axis=0, keepdims=True)\n",
        "        seq = (seq - mean) / (std + 1e-6)\n",
        "\n",
        "        seq_tensor = torch.from_numpy(seq)\n",
        "        label_tensor = torch.tensor(label)\n",
        "        return seq_tensor, label\n",
        "\n",
        "\n",
        "# Load dataset\n",
        "print(\"Creating dataset\")\n",
        "train_ds = KeypointDataset(TRAIN_DATASET_DIR)\n",
        "\n",
        "# Use the class mapping from the training set for validation set\n",
        "val_class_map = {'class_to_idx': train_ds.class_to_idx, 'idx_to_class': train_ds.idx_to_class}\n",
        "val_ds = KeypointDataset(VAL_DATASET_DIR, class_map=val_class_map)\n",
        "\n",
        "# Dataloaders\n",
        "print(\"Creating dataloaders\")\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "class_names = train_ds.idx_to_class\n",
        "num_classes = len(class_names)\n",
        "\n",
        "print(\"Classes:\", class_names[:10], \"...\")\n",
        "print(\"num_classes =\", num_classes)\n",
        "\n",
        "print(\"train_ds size:\", len(train_ds))\n",
        "print(\"val_ds size:\", len(val_ds))\n",
        "\n",
        "random_sample = train_ds[0]\n",
        "print(\"sample: \", random_sample[0])\n",
        "print(\"sample shape:\", random_sample[0].shape)\n",
        "print(\"sample unique vals: \", random_sample[0].unique())\n",
        "print(\"sample range: \", random_sample[0].max() - random_sample[0].min())\n",
        "print(\"sample label:\", random_sample[1])\n"
      ],
      "metadata": {
        "id": "X3wRmbtoDcE9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05921ee2-5566-416f-ba1f-4804f3c14dd1"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using CPU\n",
            "Creating dataset\n",
            "Creating dataloaders\n",
            "Classes: ['accident', 'africa', 'all', 'apple', 'basketball', 'bed', 'before', 'bird', 'birthday', 'black'] ...\n",
            "num_classes = 99\n",
            "train_ds size: 2853\n",
            "val_ds size: 195\n",
            "sample:  tensor([[-0.3365, -0.6419, -3.3330,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.3184, -0.7361, -0.4031,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.3156, -0.7517, -0.2711,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        ...,\n",
            "        [-1.1453, -1.2605,  0.3488,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [-1.0484, -1.2596,  0.5725,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.9326, -1.2593,  0.6848,  ...,  0.0000,  0.0000,  0.0000]])\n",
            "sample shape: torch.Size([64, 225])\n",
            "sample unique vals:  tensor([-6.1446, -5.5747, -5.4204,  ...,  4.7186,  5.0134,  5.2563])\n",
            "sample range:  tensor(11.4009)\n",
            "sample label: 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Training"
      ],
      "metadata": {
        "id": "nMbdsztyDmGn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import os\n",
        "from torchvision.models.video import r3d_18\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "\n",
        "def plot_losses(train_losses: list, val_losses: list):\n",
        "    epochs = range(1, len(train_losses) + 1)\n",
        "    plt.plot(epochs, train_losses, label='Training Loss')\n",
        "    plt.plot(epochs, val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss Curves')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Load pretrained 3D CNN\n",
        "num_classes = len(class_names)\n",
        "model = r3d_18(weights=\"KINETICS400_V1\")\n",
        "model.fc = nn.Sequential(\n",
        "    nn.Dropout(p=0.5),\n",
        "    nn.Linear(model.fc.in_features, num_classes)\n",
        ")\n",
        "model = model.to(device)\n",
        "\n",
        "# Hyperparameters\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "EPOCHS = 8\n",
        "start_epoch = 0\n",
        "run = 0\n",
        "\n",
        "# Load saved model from checkpoint\n",
        "drive.mount('/content/drive')\n",
        "CHECKPOINT_PATH = f\"/content/drive/MyDrive/3d_cnn_asl_checkpoint_run{run}.pt\"\n",
        "MODEL_PATH = f\"/content/drive/MyDrive/3d_cnn_asl_run{run}.pt\"\n",
        "LABELS_PATH = f\"/content/drive/MyDrive/3d_cnn_asl_labels_run{run}.json\"\n",
        "\n",
        "if os.path.exists(CHECKPOINT_PATH):\n",
        "    print(\"Loading checkpoint\")\n",
        "    checkpoint = torch.load(CHECKPOINT_PATH, map_location=device)\n",
        "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "    start_epoch = checkpoint[\"epoch\"] + 1\n",
        "    train_losses = checkpoint[\"train_losses\"]\n",
        "    val_losses = checkpoint[\"val_losses\"]\n",
        "    print(f\"Resuming from epoch {start_epoch}\")\n",
        "else:\n",
        "    train_losses, val_losses = [], []\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(start_epoch, EPOCHS):\n",
        "    model.train()\n",
        "    epoch_loss, correct, total = 0, 0, 0\n",
        "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
        "\n",
        "    for images, labels in pbar:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        _, preds = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += preds.eq(labels).sum().item()\n",
        "        pbar.set_postfix(loss=loss.item(), acc=100.*correct/total)\n",
        "\n",
        "    train_losses.append(epoch_loss / len(train_loader))\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_epoch_loss, val_correct, val_total = 0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_epoch_loss += loss.item()\n",
        "            _, preds = outputs.max(1)\n",
        "            val_total += labels.size(0)\n",
        "            val_correct += preds.eq(labels).sum().item()\n",
        "    val_losses.append(val_epoch_loss / len(val_loader))\n",
        "    val_acc = 100. * val_correct / val_total\n",
        "\n",
        "    print(f\"Epoch {epoch+1}: Train Acc={100.*correct/total:.2f}% | Val Acc={val_acc:.2f}%\")\n",
        "\n",
        "    # Save checkpoint every epoch\n",
        "    checkpoint = {\n",
        "        \"epoch\": epoch,\n",
        "        \"model_state_dict\": model.state_dict(),\n",
        "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "        \"train_losses\": train_losses,\n",
        "        \"val_losses\": val_losses\n",
        "    }\n",
        "    torch.save(checkpoint, CHECKPOINT_PATH)\n",
        "    print(f\"Checkpoint saved at epoch {epoch+1}\")\n",
        "\n",
        "\n",
        "\n",
        "torch.save(model.state_dict(), MODEL_PATH)\n",
        "with open(LABELS_PATH, \"w\") as f:\n",
        "    json.dump(class_names, f)\n",
        "print(\"Final model saved to:\", MODEL_PATH)"
      ],
      "metadata": {
        "id": "lZ4kqWpHDjk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing Model"
      ],
      "metadata": {
        "id": "M0_jxmEuDqk0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import tqdm\n",
        "import os\n",
        "import torch.nn as nn\n",
        "from torchvision.models.video import r3d_18\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "\n",
        "IMG_SIZE = 112\n",
        "BATCH_SIZE = 4\n",
        "target_frames = 60\n",
        "run = 0\n",
        "\n",
        "# choose device\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "    print(\"Using GPU\")\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "    print(\"Using CPU\")\n",
        "\n",
        "# Test dataset augmentations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.43216, 0.394666, 0.37645], std=[0.22803, 0.22145, 0.216989]),\n",
        "    #transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
        "])\n",
        "\n",
        "\n",
        "# Test dataset that only uses words the model trained on\n",
        "class VideoDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None, class_map=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.samples = [] # stores (video_path, label_idx) tuples\n",
        "\n",
        "        if class_map is None:\n",
        "            self.class_to_idx = {}\n",
        "            self.idx_to_class = []\n",
        "            label_folders = sorted([d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))])\n",
        "            for i, label_name in enumerate(label_folders):\n",
        "                self.class_to_idx[label_name] = i\n",
        "                self.idx_to_class.append(label_name)\n",
        "        else:\n",
        "            self.class_to_idx = class_map['class_to_idx']\n",
        "            self.idx_to_class = class_map['idx_to_class']\n",
        "\n",
        "        # Populate samples and create class mappings\n",
        "        for label_name in os.listdir(root_dir):\n",
        "            if label_name in self.class_to_idx: # Only include labels present in the class_to_idx map\n",
        "                label_path = os.path.join(root_dir, label_name)\n",
        "                if os.path.isdir(label_path):\n",
        "                    label_idx = self.class_to_idx[label_name]\n",
        "                    for video_file in os.listdir(label_path):\n",
        "                        if video_file.lower().endswith('.mp4'):\n",
        "                            video_path = os.path.join(label_path, video_file)\n",
        "                            self.samples.append((video_path, label_idx))\n",
        "\n",
        "        self.class_names = self.idx_to_class\n",
        "        print(f\"Found {len(self.samples)} video samples across {len(set(label for _, label in self.samples))} classes in {root_dir}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video_path, label = self.samples[idx]\n",
        "        frames = self._load_all_frames(video_path)\n",
        "\n",
        "        # Apply transformations to each frame\n",
        "        transformed_frames = []\n",
        "        for frame in frames:\n",
        "            pil_image = Image.fromarray(frame)\n",
        "            if self.transform:\n",
        "                transformed_frames.append(self.transform(pil_image))\n",
        "\n",
        "        frames_tensor = torch.stack(transformed_frames) # Shape: (T, C, H, W)\n",
        "        frames_tensor = frames_tensor.permute(1, 0, 2, 3) # Shape: (C, T, H, W)\n",
        "\n",
        "        return frames_tensor, label\n",
        "\n",
        "    def _load_all_frames(self, video_path):\n",
        "      cap = cv2.VideoCapture(video_path)\n",
        "      if not cap.isOpened():\n",
        "          print(f\"Error: Could not open video file: {video_path}\")\n",
        "          return []\n",
        "\n",
        "      extracted_frames_rgb = []\n",
        "      while True:\n",
        "          ret, frame = cap.read()\n",
        "          if not ret:\n",
        "              break\n",
        "          rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "          extracted_frames_rgb.append(rgb_frame)\n",
        "\n",
        "      cap.release()\n",
        "\n",
        "      if len(extracted_frames_rgb) == 0:\n",
        "          print(f\"Warning: No frames extracted from video {video_path}\")\n",
        "\n",
        "      return extracted_frames_rgb\n",
        "\n",
        "\n",
        "# Config\n",
        "test_dir = f\"/content/drive/MyDrive/WLASL/WLASL100_test_{target_frames}frames\"\n",
        "LABELS_PATH = f\"/content/drive/MyDrive/3d_cnn_asl_labels_run{run}.json\"\n",
        "MODEL_PATH = f\"/content/drive/MyDrive/3d_cnn_asl_checkpoint_run{run}.pt\"\n",
        "\n",
        "\n",
        "# Build labels list\n",
        "with open(LABELS_PATH, \"r\") as f:\n",
        "    class_names = json.load(f)\n",
        "class_to_idx = {c: i for i, c in enumerate(class_names)}\n",
        "num_classes = len(class_names)\n",
        "\n",
        "train_labels = set(class_names)\n",
        "test_labels = {d.name for d in Path(test_dir).iterdir() if d.is_dir()}\n",
        "in_train_and_test = sorted(list(train_labels & test_labels))\n",
        "print(f\"Testing on {len(in_train_and_test)} shared classes between training & testing\")\n",
        "\n",
        "# Create class_map\n",
        "test_class_map = {'class_to_idx': class_to_idx, 'idx_to_class': class_names}\n",
        "\n",
        "# Initialize dataset, dataloader, and model\n",
        "test_dataset = VideoDataset(test_dir, transform=transform, class_map=test_class_map)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "model = r3d_18(weights=\"KINETICS400_V1\")\n",
        "model.fc = nn.Sequential(\n",
        "    nn.Dropout(p=0.5),\n",
        "    nn.Linear(model.fc.in_features, num_classes)\n",
        ")\n",
        "\n",
        "# Load model\n",
        "checkpoint = torch.load(MODEL_PATH, map_location=device)\n",
        "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Testing\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "with torch.no_grad():\n",
        "    for images, labels in tqdm.tqdm(test_loader, desc=\"Testing\"):\n",
        "        images = images.to(device)\n",
        "        outputs = model(images)\n",
        "        _, preds = outputs.max(1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Compute & display metrics\n",
        "accuracy = accuracy_score(all_labels, all_preds)\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# cm = confusion_matrix(all_labels, all_preds)\n",
        "# print(\"Confusion Matrix:\")\n",
        "# print(cm)\n",
        "\n",
        "# disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
        "# disp.plot(xticks_rotation='vertical', cmap='Blues')\n",
        "# plt.title(\"Confusion Matrix\")\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "KI8Zl1AbDnfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing Model"
      ],
      "metadata": {
        "id": "Rf_oP0kTHWxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import tqdm\n",
        "import os\n",
        "import torch.nn as nn\n",
        "from torchvision.models.video import r3d_18\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "\n",
        "IMG_SIZE = 112\n",
        "BATCH_SIZE = 4\n",
        "target_frames = 60\n",
        "run = 4\n",
        "\n",
        "# choose device\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "    print(\"Using GPU\")\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "    print(\"Using CPU\")\n",
        "\n",
        "# Test dataset augmentations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.43216, 0.394666, 0.37645], std=[0.22803, 0.22145, 0.216989]),\n",
        "    #transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
        "])\n",
        "\n",
        "\n",
        "# Test dataset that only uses words the model trained on\n",
        "class VideoDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None, class_map=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.samples = [] # stores (video_path, label_idx) tuples\n",
        "\n",
        "        if class_map is None:\n",
        "            self.class_to_idx = {}\n",
        "            self.idx_to_class = []\n",
        "            label_folders = sorted([d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))])\n",
        "            for i, label_name in enumerate(label_folders):\n",
        "                self.class_to_idx[label_name] = i\n",
        "                self.idx_to_class.append(label_name)\n",
        "        else:\n",
        "            self.class_to_idx = class_map['class_to_idx']\n",
        "            self.idx_to_class = class_map['idx_to_class']\n",
        "\n",
        "        # Populate samples and create class mappings\n",
        "        for label_name in os.listdir(root_dir):\n",
        "            if label_name in self.class_to_idx: # Only include labels present in the class_to_idx map\n",
        "                label_path = os.path.join(root_dir, label_name)\n",
        "                if os.path.isdir(label_path):\n",
        "                    label_idx = self.class_to_idx[label_name]\n",
        "                    for video_file in os.listdir(label_path):\n",
        "                        if video_file.lower().endswith('.mp4'):\n",
        "                            video_path = os.path.join(label_path, video_file)\n",
        "                            self.samples.append((video_path, label_idx))\n",
        "\n",
        "        self.class_names = self.idx_to_class\n",
        "        print(f\"Found {len(self.samples)} video samples across {len(set(label for _, label in self.samples))} classes in {root_dir}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video_path, label = self.samples[idx]\n",
        "        frames = self._load_all_frames(video_path)\n",
        "\n",
        "        # Apply transformations to each frame\n",
        "        transformed_frames = []\n",
        "        for frame in frames:\n",
        "            pil_image = Image.fromarray(frame)\n",
        "            if self.transform:\n",
        "                transformed_frames.append(self.transform(pil_image))\n",
        "\n",
        "        frames_tensor = torch.stack(transformed_frames) # Shape: (T, C, H, W)\n",
        "        frames_tensor = frames_tensor.permute(1, 0, 2, 3) # Shape: (C, T, H, W)\n",
        "\n",
        "        return frames_tensor, label\n",
        "\n",
        "    def _load_all_frames(self, video_path):\n",
        "      cap = cv2.VideoCapture(video_path)\n",
        "      if not cap.isOpened():\n",
        "          print(f\"Error: Could not open video file: {video_path}\")\n",
        "          return []\n",
        "\n",
        "      extracted_frames_rgb = []\n",
        "      while True:\n",
        "          ret, frame = cap.read()\n",
        "          if not ret:\n",
        "              break\n",
        "          rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "          extracted_frames_rgb.append(rgb_frame)\n",
        "\n",
        "      cap.release()\n",
        "\n",
        "      if len(extracted_frames_rgb) == 0:\n",
        "          print(f\"Warning: No frames extracted from video {video_path}\")\n",
        "\n",
        "      return extracted_frames_rgb\n",
        "\n",
        "\n",
        "# Config\n",
        "test_dir = f\"/content/drive/MyDrive/WLASL/WLASL100_test_{target_frames}frames\"\n",
        "LABELS_PATH = f\"/content/drive/MyDrive/3d_cnn_asl_labels_run{run}.json\"\n",
        "MODEL_PATH = f\"/content/drive/MyDrive/3d_cnn_asl_checkpoint_run{run}.pt\"\n",
        "\n",
        "\n",
        "# Build labels list\n",
        "with open(LABELS_PATH, \"r\") as f:\n",
        "    class_names = json.load(f)\n",
        "class_to_idx = {c: i for i, c in enumerate(class_names)}\n",
        "num_classes = len(class_names)\n",
        "\n",
        "train_labels = set(class_names)\n",
        "test_labels = {d.name for d in Path(test_dir).iterdir() if d.is_dir()}\n",
        "in_train_and_test = sorted(list(train_labels & test_labels))\n",
        "print(f\"Testing on {len(in_train_and_test)} shared classes between training & testing\")\n",
        "\n",
        "# Create class_map\n",
        "test_class_map = {'class_to_idx': class_to_idx, 'idx_to_class': class_names}\n",
        "\n",
        "# Initialize dataset, dataloader, and model\n",
        "test_dataset = VideoDataset(test_dir, transform=transform, class_map=test_class_map)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "model = r3d_18(weights=\"KINETICS400_V1\")\n",
        "model.fc = nn.Sequential(\n",
        "    nn.Dropout(p=0.5),\n",
        "    nn.Linear(model.fc.in_features, num_classes)\n",
        ")\n",
        "\n",
        "# Load model\n",
        "checkpoint = torch.load(MODEL_PATH, map_location=device)\n",
        "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Testing\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "with torch.no_grad():\n",
        "    for images, labels in tqdm.tqdm(test_loader, desc=\"Testing\"):\n",
        "        images = images.to(device)\n",
        "        outputs = model(images)\n",
        "        _, preds = outputs.max(1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Compute & display metrics\n",
        "accuracy = accuracy_score(all_labels, all_preds)\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# cm = confusion_matrix(all_labels, all_preds)\n",
        "# print(\"Confusion Matrix:\")\n",
        "# print(cm)\n",
        "\n",
        "# disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
        "# disp.plot(xticks_rotation='vertical', cmap='Blues')\n",
        "# plt.title(\"Confusion Matrix\")\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "wykKGrZKHXv2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}