{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alimomennasab/ASL-Translator/blob/main/CS4200_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bGBv3Zg_CvdV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8ac783d-7d62-4d80-8675-5db5f66b19df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Preprocessing"
      ],
      "metadata": {
        "id": "XHeb0AUBKToG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mediapipe"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K20yTQ9n-UaY",
        "outputId": "53d3b7f0-cc61-4070-cf0d-e18bbfec6dcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mediapipe in /usr/local/lib/python3.12/dist-packages (0.10.21)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (25.4.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (25.9.23)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.7.1)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.7.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from mediapipe) (3.10.0)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (1.26.4)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.12/dist-packages (from mediapipe) (4.11.0.86)\n",
            "Requirement already satisfied: protobuf<5,>=4.25.3 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (4.25.8)\n",
            "Requirement already satisfied: sounddevice>=0.4.4 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.5.3)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.2.1)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.12/dist-packages (from sounddevice>=0.4.4->mediapipe) (2.0.0)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe) (0.5.4)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.12 in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe) (1.16.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (2.9.0.post0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.23)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# extract keypoints from videos\n",
        "\n",
        "import tqdm\n",
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "# take the results of holistic.process on a frame and extract keypoints\n",
        "def extract_keypoint_vector(results):\n",
        "    keypoints = []\n",
        "\n",
        "    # Pose\n",
        "    if results.pose_landmarks:\n",
        "        for lm in results.pose_landmarks.landmark:\n",
        "            keypoints.extend([lm.x, lm.y, lm.z])\n",
        "    else:\n",
        "        keypoints.extend([0.0] * 33 * 3)\n",
        "\n",
        "    # Left hand\n",
        "    if results.left_hand_landmarks:\n",
        "        for lm in results.left_hand_landmarks.landmark:\n",
        "            keypoints.extend([lm.x, lm.y, lm.z])\n",
        "    else:\n",
        "        keypoints.extend([0.0] * 21 * 3)\n",
        "\n",
        "    # Right hand\n",
        "    if results.right_hand_landmarks:\n",
        "        for lm in results.right_hand_landmarks.landmark:\n",
        "            keypoints.extend([lm.x, lm.y, lm.z])\n",
        "    else:\n",
        "        keypoints.extend([0.0] * 21 * 3)\n",
        "\n",
        "    return np.array(keypoints, dtype=np.float32) # (75*3,)\n",
        "\n",
        "\n",
        "# go through each frame of a video and perform keypoint extraction\n",
        "def extract_video_keypoints(vid_path, holistic):\n",
        "    # convert video to frames\n",
        "    cap = cv2.VideoCapture(os.path.join(vid_path))\n",
        "    frames = []\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frames.append(frame)\n",
        "    cap.release()\n",
        "\n",
        "    kp_seq = []\n",
        "\n",
        "    for frame in frames:\n",
        "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        results = holistic.process(rgb)\n",
        "        kp_vec = extract_keypoint_vector(results)\n",
        "        kp_seq.append(kp_vec)\n",
        "\n",
        "    return np.stack(kp_seq, axis=0)\n",
        "\n",
        "\n",
        "# Processing entire dataset\n",
        "def process_dataset(dataset_dir, output_dir):\n",
        "    mp_holistic = mp.solutions.holistic\n",
        "    with mp_holistic.Holistic(\n",
        "        static_image_mode=False,\n",
        "        model_complexity=1,\n",
        "        enable_segmentation=False,\n",
        "        refine_face_landmarks=False\n",
        "    ) as holistic:\n",
        "\n",
        "      for label in tqdm.tqdm(os.listdir(dataset_dir)):\n",
        "          label_dir = os.path.join(dataset_dir, label)\n",
        "          output_label_dir = os.path.join(output_dir, label)\n",
        "          os.makedirs(output_label_dir, exist_ok=True)\n",
        "\n",
        "          for video in os.listdir(label_dir):\n",
        "              video_path = os.path.join(label_dir, video)\n",
        "              base = os.path.splitext(video)[0]\n",
        "              output_video_path = os.path.join(output_label_dir, base + \".npy\")\n",
        "\n",
        "              kp_seq = extract_video_keypoints(video_path, holistic)\n",
        "              np.save(output_video_path, kp_seq)\n",
        "\n",
        "# Main processing\n",
        "target_frames = 64\n",
        "\n",
        "TRAIN_DATASET_DIR = f\"/content/drive/MyDrive/WLASL/WLASL100_train_augmented_{target_frames}frames\"\n",
        "VAL_DATASET_DIR = f'/content/drive/MyDrive/WLASL/WLASL100_val_{target_frames}frames'\n",
        "TEST_DATASET_DIR = f'/content/drive/MyDrive/WLASL/WLASL100_test_{target_frames}frames'\n",
        "\n",
        "KP_TRAIN_DATASET_DIR = f\"/content/drive/MyDrive/WLASL/WLASL100_KP_train_augmented_{target_frames}\"\n",
        "KP_VAL_DATASET_DIR = f'/content/drive/MyDrive/WLASL/WLASL100_KP_val_{target_frames}frames'\n",
        "KP_TEST_DATASET_DIR = f'/content/drive/MyDrive/WLASL/WLASL100_KP_test_{target_frames}frames'\n",
        "os.makedirs(KP_TRAIN_DATASET_DIR, exist_ok=True)\n",
        "os.makedirs(KP_VAL_DATASET_DIR, exist_ok=True)\n",
        "os.makedirs(KP_TEST_DATASET_DIR, exist_ok=True)\n",
        "\n",
        "#process_dataset(dataset_dir=TRAIN_DATASET_DIR, output_dir=KP_TRAIN_DATASET_DIR)\n",
        "#process_dataset(dataset_dir=VAL_DATASET_DIR, output_dir=KP_VAL_DATASET_DIR)\n",
        "process_dataset(dataset_dir=TEST_DATASET_DIR, output_dir= KP_TEST_DATASET_DIR)"
      ],
      "metadata": {
        "id": "kvmV0EmWKUu6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b2686ac-a418-4272-ba8a-9480c754e0fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 86/86 [07:57<00:00,  5.55s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Loading"
      ],
      "metadata": {
        "id": "AsozKp16DeOu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "import cv2\n",
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "\n",
        "target_frames = 64\n",
        "TRAIN_DATASET_DIR = f\"/content/drive/MyDrive/WLASL/WLASL100_train_augmented_{target_frames}frames\"\n",
        "VAL_DATASET_DIR = f'/content/drive/MyDrive/WLASL/WLASL100_val_{target_frames}frames'\n",
        "IMG_SIZE = 112\n",
        "BATCH_SIZE = 4\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "    print(\"Using GPU\")\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "    print(\"Using CPU\")\n",
        "\n",
        "# Image transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.43216, 0.394666, 0.37645], std=[0.22803, 0.22145, 0.216989]),\n",
        "    #transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
        "])\n",
        "\n",
        "\n",
        "class VideoDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None, class_map=None, num_frames=16, mode=\"Train\"):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.num_frames = num_frames\n",
        "        self.mode = mode\n",
        "        self.samples = [] # stores (video_path, label_idx) tuples\n",
        "\n",
        "        if class_map is None:\n",
        "            self.class_to_idx = {}\n",
        "            self.idx_to_class = []\n",
        "            label_folders = sorted([d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))])\n",
        "            for i, label_name in enumerate(label_folders):\n",
        "                self.class_to_idx[label_name] = i\n",
        "                self.idx_to_class.append(label_name)\n",
        "        else:\n",
        "            self.class_to_idx = class_map['class_to_idx']\n",
        "            self.idx_to_class = class_map['idx_to_class']\n",
        "\n",
        "        # Populate samples and create class mappings\n",
        "        for label_name in os.listdir(root_dir):\n",
        "            if label_name in self.class_to_idx: # Only include labels present in the class_to_idx map\n",
        "                label_path = os.path.join(root_dir, label_name)\n",
        "                if os.path.isdir(label_path):\n",
        "                    label_idx = self.class_to_idx[label_name]\n",
        "                    for video_file in os.listdir(label_path):\n",
        "                        if video_file.lower().endswith('.mp4'):\n",
        "                            video_path = os.path.join(label_path, video_file)\n",
        "                            self.samples.append((video_path, label_idx))\n",
        "\n",
        "        self.class_names = self.idx_to_class\n",
        "        print(f\"Found {len(self.samples)} video samples across {len(set(label for _, label in self.samples))} classes in {root_dir}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def random_sample(self, frames):\n",
        "      max_start = len(frames) - self.num_frames\n",
        "      start = np.random.randint(0, max_start + 1)\n",
        "      return frames[start:start + self.num_frames]\n",
        "\n",
        "    def uniform_sample(self, frames):\n",
        "      idxs = np.linspace(0, len(frames) - 1, self.num_frames).astype(int)\n",
        "      return [frames[i] for i in idxs]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video_path, label = self.samples[idx]\n",
        "        frames = self._load_all_frames(video_path)\n",
        "\n",
        "        assert self.mode == \"Train\" or self.mode == \"Val\", \"Mode must be either 'Train' or 'Val'\"\n",
        "\n",
        "        if self.mode == \"Train\":\n",
        "          frames = self.random_sample(frames)\n",
        "        elif self.mode == \"Val\":\n",
        "          frames = self.uniform_sample(frames)\n",
        "\n",
        "        # Apply transformations to each frame\n",
        "        transformed_frames = []\n",
        "        for frame in frames:\n",
        "            pil_image = Image.fromarray(frame)\n",
        "            if self.transform:\n",
        "                transformed_frames.append(self.transform(pil_image))\n",
        "\n",
        "        frames_tensor = torch.stack(transformed_frames) # Shape: (T, C, H, W)\n",
        "        frames_tensor = frames_tensor.permute(1, 0, 2, 3) # Shape: (C, T, H, W)\n",
        "\n",
        "        return frames_tensor, label\n",
        "\n",
        "    def _load_all_frames(self, video_path):\n",
        "      cap = cv2.VideoCapture(video_path)\n",
        "      if not cap.isOpened():\n",
        "          print(f\"Error: Could not open video file: {video_path}\")\n",
        "          return []\n",
        "\n",
        "      extracted_frames_rgb = []\n",
        "      while True:\n",
        "          ret, frame = cap.read()\n",
        "          if not ret:\n",
        "              break\n",
        "          rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "          extracted_frames_rgb.append(rgb_frame)\n",
        "\n",
        "      cap.release()\n",
        "\n",
        "      if len(extracted_frames_rgb) == 0:\n",
        "          print(f\"Warning: No frames extracted from video {video_path}\")\n",
        "\n",
        "      return extracted_frames_rgb\n",
        "\n",
        "\n",
        "\n",
        "# Load dataset\n",
        "print(\"Creating dataset\")\n",
        "train_ds = VideoDataset(TRAIN_DATASET_DIR, transform=transform, num_frames=16, mode=\"Train\")\n",
        "\n",
        "# Use the class mapping from the training set for validation set\n",
        "val_class_map = {'class_to_idx': train_ds.class_to_idx, 'idx_to_class': train_ds.idx_to_class}\n",
        "val_ds = VideoDataset(VAL_DATASET_DIR, transform=transform, class_map=val_class_map, num_frames=16, mode=\"Val\")\n",
        "\n",
        "# Dataloaders\n",
        "print(\"Creating dataloaders\")\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "class_names = train_ds.class_names\n",
        "num_classes = len(class_names)\n",
        "\n",
        "print(\"Classes:\", class_names[:10], \"...\")\n",
        "print(\"num_classes =\", num_classes)\n"
      ],
      "metadata": {
        "id": "X3wRmbtoDcE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Training"
      ],
      "metadata": {
        "id": "nMbdsztyDmGn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import os\n",
        "from torchvision.models.video import r3d_18\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "\n",
        "def plot_losses(train_losses: list, val_losses: list):\n",
        "    epochs = range(1, len(train_losses) + 1)\n",
        "    plt.plot(epochs, train_losses, label='Training Loss')\n",
        "    plt.plot(epochs, val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss Curves')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Load pretrained 3D CNN\n",
        "num_classes = len(class_names)\n",
        "model = r3d_18(weights=\"KINETICS400_V1\")\n",
        "model.fc = nn.Sequential(\n",
        "    nn.Dropout(p=0.5),\n",
        "    nn.Linear(model.fc.in_features, num_classes)\n",
        ")\n",
        "model = model.to(device)\n",
        "\n",
        "# Hyperparameters\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "EPOCHS = 8\n",
        "start_epoch = 0\n",
        "run = 0\n",
        "\n",
        "# Load saved model from checkpoint\n",
        "drive.mount('/content/drive')\n",
        "CHECKPOINT_PATH = f\"/content/drive/MyDrive/3d_cnn_asl_checkpoint_run{run}.pt\"\n",
        "MODEL_PATH = f\"/content/drive/MyDrive/3d_cnn_asl_run{run}.pt\"\n",
        "LABELS_PATH = f\"/content/drive/MyDrive/3d_cnn_asl_labels_run{run}.json\"\n",
        "\n",
        "if os.path.exists(CHECKPOINT_PATH):\n",
        "    print(\"Loading checkpoint\")\n",
        "    checkpoint = torch.load(CHECKPOINT_PATH, map_location=device)\n",
        "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "    start_epoch = checkpoint[\"epoch\"] + 1\n",
        "    train_losses = checkpoint[\"train_losses\"]\n",
        "    val_losses = checkpoint[\"val_losses\"]\n",
        "    print(f\"Resuming from epoch {start_epoch}\")\n",
        "else:\n",
        "    train_losses, val_losses = [], []\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(start_epoch, EPOCHS):\n",
        "    model.train()\n",
        "    epoch_loss, correct, total = 0, 0, 0\n",
        "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
        "\n",
        "    for images, labels in pbar:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        _, preds = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += preds.eq(labels).sum().item()\n",
        "        pbar.set_postfix(loss=loss.item(), acc=100.*correct/total)\n",
        "\n",
        "    train_losses.append(epoch_loss / len(train_loader))\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_epoch_loss, val_correct, val_total = 0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_epoch_loss += loss.item()\n",
        "            _, preds = outputs.max(1)\n",
        "            val_total += labels.size(0)\n",
        "            val_correct += preds.eq(labels).sum().item()\n",
        "    val_losses.append(val_epoch_loss / len(val_loader))\n",
        "    val_acc = 100. * val_correct / val_total\n",
        "\n",
        "    print(f\"Epoch {epoch+1}: Train Acc={100.*correct/total:.2f}% | Val Acc={val_acc:.2f}%\")\n",
        "\n",
        "    # Save checkpoint every epoch\n",
        "    checkpoint = {\n",
        "        \"epoch\": epoch,\n",
        "        \"model_state_dict\": model.state_dict(),\n",
        "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "        \"train_losses\": train_losses,\n",
        "        \"val_losses\": val_losses\n",
        "    }\n",
        "    torch.save(checkpoint, CHECKPOINT_PATH)\n",
        "    print(f\"Checkpoint saved at epoch {epoch+1}\")\n",
        "\n",
        "\n",
        "\n",
        "torch.save(model.state_dict(), MODEL_PATH)\n",
        "with open(LABELS_PATH, \"w\") as f:\n",
        "    json.dump(class_names, f)\n",
        "print(\"Final model saved to:\", MODEL_PATH)"
      ],
      "metadata": {
        "id": "lZ4kqWpHDjk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing Model"
      ],
      "metadata": {
        "id": "M0_jxmEuDqk0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import tqdm\n",
        "import os\n",
        "import torch.nn as nn\n",
        "from torchvision.models.video import r3d_18\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "\n",
        "IMG_SIZE = 112\n",
        "BATCH_SIZE = 4\n",
        "target_frames = 60\n",
        "run = 0\n",
        "\n",
        "# choose device\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "    print(\"Using GPU\")\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "    print(\"Using CPU\")\n",
        "\n",
        "# Test dataset augmentations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.43216, 0.394666, 0.37645], std=[0.22803, 0.22145, 0.216989]),\n",
        "    #transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
        "])\n",
        "\n",
        "\n",
        "# Test dataset that only uses words the model trained on\n",
        "class VideoDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None, class_map=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.samples = [] # stores (video_path, label_idx) tuples\n",
        "\n",
        "        if class_map is None:\n",
        "            self.class_to_idx = {}\n",
        "            self.idx_to_class = []\n",
        "            label_folders = sorted([d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))])\n",
        "            for i, label_name in enumerate(label_folders):\n",
        "                self.class_to_idx[label_name] = i\n",
        "                self.idx_to_class.append(label_name)\n",
        "        else:\n",
        "            self.class_to_idx = class_map['class_to_idx']\n",
        "            self.idx_to_class = class_map['idx_to_class']\n",
        "\n",
        "        # Populate samples and create class mappings\n",
        "        for label_name in os.listdir(root_dir):\n",
        "            if label_name in self.class_to_idx: # Only include labels present in the class_to_idx map\n",
        "                label_path = os.path.join(root_dir, label_name)\n",
        "                if os.path.isdir(label_path):\n",
        "                    label_idx = self.class_to_idx[label_name]\n",
        "                    for video_file in os.listdir(label_path):\n",
        "                        if video_file.lower().endswith('.mp4'):\n",
        "                            video_path = os.path.join(label_path, video_file)\n",
        "                            self.samples.append((video_path, label_idx))\n",
        "\n",
        "        self.class_names = self.idx_to_class\n",
        "        print(f\"Found {len(self.samples)} video samples across {len(set(label for _, label in self.samples))} classes in {root_dir}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video_path, label = self.samples[idx]\n",
        "        frames = self._load_all_frames(video_path)\n",
        "\n",
        "        # Apply transformations to each frame\n",
        "        transformed_frames = []\n",
        "        for frame in frames:\n",
        "            pil_image = Image.fromarray(frame)\n",
        "            if self.transform:\n",
        "                transformed_frames.append(self.transform(pil_image))\n",
        "\n",
        "        frames_tensor = torch.stack(transformed_frames) # Shape: (T, C, H, W)\n",
        "        frames_tensor = frames_tensor.permute(1, 0, 2, 3) # Shape: (C, T, H, W)\n",
        "\n",
        "        return frames_tensor, label\n",
        "\n",
        "    def _load_all_frames(self, video_path):\n",
        "      cap = cv2.VideoCapture(video_path)\n",
        "      if not cap.isOpened():\n",
        "          print(f\"Error: Could not open video file: {video_path}\")\n",
        "          return []\n",
        "\n",
        "      extracted_frames_rgb = []\n",
        "      while True:\n",
        "          ret, frame = cap.read()\n",
        "          if not ret:\n",
        "              break\n",
        "          rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "          extracted_frames_rgb.append(rgb_frame)\n",
        "\n",
        "      cap.release()\n",
        "\n",
        "      if len(extracted_frames_rgb) == 0:\n",
        "          print(f\"Warning: No frames extracted from video {video_path}\")\n",
        "\n",
        "      return extracted_frames_rgb\n",
        "\n",
        "\n",
        "# Config\n",
        "test_dir = f\"/content/drive/MyDrive/WLASL/WLASL100_test_{target_frames}frames\"\n",
        "LABELS_PATH = f\"/content/drive/MyDrive/3d_cnn_asl_labels_run{run}.json\"\n",
        "MODEL_PATH = f\"/content/drive/MyDrive/3d_cnn_asl_checkpoint_run{run}.pt\"\n",
        "\n",
        "\n",
        "# Build labels list\n",
        "with open(LABELS_PATH, \"r\") as f:\n",
        "    class_names = json.load(f)\n",
        "class_to_idx = {c: i for i, c in enumerate(class_names)}\n",
        "num_classes = len(class_names)\n",
        "\n",
        "train_labels = set(class_names)\n",
        "test_labels = {d.name for d in Path(test_dir).iterdir() if d.is_dir()}\n",
        "in_train_and_test = sorted(list(train_labels & test_labels))\n",
        "print(f\"Testing on {len(in_train_and_test)} shared classes between training & testing\")\n",
        "\n",
        "# Create class_map\n",
        "test_class_map = {'class_to_idx': class_to_idx, 'idx_to_class': class_names}\n",
        "\n",
        "# Initialize dataset, dataloader, and model\n",
        "test_dataset = VideoDataset(test_dir, transform=transform, class_map=test_class_map)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "model = r3d_18(weights=\"KINETICS400_V1\")\n",
        "model.fc = nn.Sequential(\n",
        "    nn.Dropout(p=0.5),\n",
        "    nn.Linear(model.fc.in_features, num_classes)\n",
        ")\n",
        "\n",
        "# Load model\n",
        "checkpoint = torch.load(MODEL_PATH, map_location=device)\n",
        "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Testing\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "with torch.no_grad():\n",
        "    for images, labels in tqdm.tqdm(test_loader, desc=\"Testing\"):\n",
        "        images = images.to(device)\n",
        "        outputs = model(images)\n",
        "        _, preds = outputs.max(1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Compute & display metrics\n",
        "accuracy = accuracy_score(all_labels, all_preds)\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# cm = confusion_matrix(all_labels, all_preds)\n",
        "# print(\"Confusion Matrix:\")\n",
        "# print(cm)\n",
        "\n",
        "# disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
        "# disp.plot(xticks_rotation='vertical', cmap='Blues')\n",
        "# plt.title(\"Confusion Matrix\")\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "KI8Zl1AbDnfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing Model"
      ],
      "metadata": {
        "id": "Rf_oP0kTHWxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import tqdm\n",
        "import os\n",
        "import torch.nn as nn\n",
        "from torchvision.models.video import r3d_18\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "\n",
        "IMG_SIZE = 112\n",
        "BATCH_SIZE = 4\n",
        "target_frames = 60\n",
        "run = 4\n",
        "\n",
        "# choose device\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "    print(\"Using GPU\")\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "    print(\"Using CPU\")\n",
        "\n",
        "# Test dataset augmentations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.43216, 0.394666, 0.37645], std=[0.22803, 0.22145, 0.216989]),\n",
        "    #transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
        "])\n",
        "\n",
        "\n",
        "# Test dataset that only uses words the model trained on\n",
        "class VideoDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None, class_map=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.samples = [] # stores (video_path, label_idx) tuples\n",
        "\n",
        "        if class_map is None:\n",
        "            self.class_to_idx = {}\n",
        "            self.idx_to_class = []\n",
        "            label_folders = sorted([d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))])\n",
        "            for i, label_name in enumerate(label_folders):\n",
        "                self.class_to_idx[label_name] = i\n",
        "                self.idx_to_class.append(label_name)\n",
        "        else:\n",
        "            self.class_to_idx = class_map['class_to_idx']\n",
        "            self.idx_to_class = class_map['idx_to_class']\n",
        "\n",
        "        # Populate samples and create class mappings\n",
        "        for label_name in os.listdir(root_dir):\n",
        "            if label_name in self.class_to_idx: # Only include labels present in the class_to_idx map\n",
        "                label_path = os.path.join(root_dir, label_name)\n",
        "                if os.path.isdir(label_path):\n",
        "                    label_idx = self.class_to_idx[label_name]\n",
        "                    for video_file in os.listdir(label_path):\n",
        "                        if video_file.lower().endswith('.mp4'):\n",
        "                            video_path = os.path.join(label_path, video_file)\n",
        "                            self.samples.append((video_path, label_idx))\n",
        "\n",
        "        self.class_names = self.idx_to_class\n",
        "        print(f\"Found {len(self.samples)} video samples across {len(set(label for _, label in self.samples))} classes in {root_dir}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video_path, label = self.samples[idx]\n",
        "        frames = self._load_all_frames(video_path)\n",
        "\n",
        "        # Apply transformations to each frame\n",
        "        transformed_frames = []\n",
        "        for frame in frames:\n",
        "            pil_image = Image.fromarray(frame)\n",
        "            if self.transform:\n",
        "                transformed_frames.append(self.transform(pil_image))\n",
        "\n",
        "        frames_tensor = torch.stack(transformed_frames) # Shape: (T, C, H, W)\n",
        "        frames_tensor = frames_tensor.permute(1, 0, 2, 3) # Shape: (C, T, H, W)\n",
        "\n",
        "        return frames_tensor, label\n",
        "\n",
        "    def _load_all_frames(self, video_path):\n",
        "      cap = cv2.VideoCapture(video_path)\n",
        "      if not cap.isOpened():\n",
        "          print(f\"Error: Could not open video file: {video_path}\")\n",
        "          return []\n",
        "\n",
        "      extracted_frames_rgb = []\n",
        "      while True:\n",
        "          ret, frame = cap.read()\n",
        "          if not ret:\n",
        "              break\n",
        "          rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "          extracted_frames_rgb.append(rgb_frame)\n",
        "\n",
        "      cap.release()\n",
        "\n",
        "      if len(extracted_frames_rgb) == 0:\n",
        "          print(f\"Warning: No frames extracted from video {video_path}\")\n",
        "\n",
        "      return extracted_frames_rgb\n",
        "\n",
        "\n",
        "# Config\n",
        "test_dir = f\"/content/drive/MyDrive/WLASL/WLASL100_test_{target_frames}frames\"\n",
        "LABELS_PATH = f\"/content/drive/MyDrive/3d_cnn_asl_labels_run{run}.json\"\n",
        "MODEL_PATH = f\"/content/drive/MyDrive/3d_cnn_asl_checkpoint_run{run}.pt\"\n",
        "\n",
        "\n",
        "# Build labels list\n",
        "with open(LABELS_PATH, \"r\") as f:\n",
        "    class_names = json.load(f)\n",
        "class_to_idx = {c: i for i, c in enumerate(class_names)}\n",
        "num_classes = len(class_names)\n",
        "\n",
        "train_labels = set(class_names)\n",
        "test_labels = {d.name for d in Path(test_dir).iterdir() if d.is_dir()}\n",
        "in_train_and_test = sorted(list(train_labels & test_labels))\n",
        "print(f\"Testing on {len(in_train_and_test)} shared classes between training & testing\")\n",
        "\n",
        "# Create class_map\n",
        "test_class_map = {'class_to_idx': class_to_idx, 'idx_to_class': class_names}\n",
        "\n",
        "# Initialize dataset, dataloader, and model\n",
        "test_dataset = VideoDataset(test_dir, transform=transform, class_map=test_class_map)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "model = r3d_18(weights=\"KINETICS400_V1\")\n",
        "model.fc = nn.Sequential(\n",
        "    nn.Dropout(p=0.5),\n",
        "    nn.Linear(model.fc.in_features, num_classes)\n",
        ")\n",
        "\n",
        "# Load model\n",
        "checkpoint = torch.load(MODEL_PATH, map_location=device)\n",
        "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Testing\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "with torch.no_grad():\n",
        "    for images, labels in tqdm.tqdm(test_loader, desc=\"Testing\"):\n",
        "        images = images.to(device)\n",
        "        outputs = model(images)\n",
        "        _, preds = outputs.max(1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Compute & display metrics\n",
        "accuracy = accuracy_score(all_labels, all_preds)\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# cm = confusion_matrix(all_labels, all_preds)\n",
        "# print(\"Confusion Matrix:\")\n",
        "# print(cm)\n",
        "\n",
        "# disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
        "# disp.plot(xticks_rotation='vertical', cmap='Blues')\n",
        "# plt.title(\"Confusion Matrix\")\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "wykKGrZKHXv2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}